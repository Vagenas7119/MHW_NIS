---
title: "stage_4_bind_mhw_records"
author: "Shahar Chaikin"
date: "2025-09-24"
output: html_document
---

Bind climatologies with records data

Libraries
```{r}
library(tidyverse)
```

#data
```{r}
#trended
 clim=read_rds("climatology_df_median_sst_10m.rds")
 events=read_rds("event_df_median_sst_10m.rds")
#detrended
clim_dt=read_rds("climatology_dt_df.rds")
events_dt=read_rds("event_dt_df.rds")

#Proxy for monitoring effort
ormef <- read.delim("C:\\Users\\User\\Desktop\\research\\data\\ORMEF\\89476.tsv") %>% 
  mutate(
    # 1. str_replace() changes all commas (",") in the string to a period (".")
    Decimal_Lat = str_replace(Decimal_Lat, ",", "."),
    Decimal_Long = str_replace(Decimal_Long, ",", ".")) %>%
  # 2. Use a second mutate() call to convert the new string format to a numeric type
  mutate(
    Decimal_Lat = as.numeric(Decimal_Lat),
    Decimal_Long = as.numeric(Decimal_Long),
    h3_id = h3jsr::point_to_cell(
    input = sf::st_as_sf(.,
                         coords = c("Decimal_Long", "Decimal_Lat"),
                         crs = 4326),
    res = 3)) %>% 
  filter(Category %in%"EXOTIC CAN")

ormef_h3_id=ormef %>% 
  group_by(h3_id) %>% 
  summarise(sources=n_distinct(Reference))

ormef_h3_id_year=ormef %>% 
  group_by(h3_id,Year) %>% 
  summarise(sources=n_distinct(Reference))

ggplot(ormef_h3_id_year)+
  geom_point(aes(x=Year,y=sources))
```

#1.1) annual cumulative intensity
Estimate annual cumulative intensity per hexagon
```{r eval=FALSE, include=FALSE}
annual_c_int=clim %>% 
  drop_na(median_sst_10m) %>% 
  mutate(
    # Create a new column, e.g., 'temp_anomaly'
    intensity = dplyr::if_else(
      !is.na(event_no), # The condition: is event_no not NA?
      median_sst_10m - seas, # What to do if the condition is TRUE
      0 # What to do if the condition is FALSE
    )) %>% 
    mutate(year_of_mhws=lubridate::year(date)) %>% 
  group_by(h3_id,year_of_mhws) %>% 
  summarise(annual_c_int=sum(intensity)) %>% 
  mutate(year_of_records=year_of_mhws+1)

#write_rds(annual_c_int,"annual_c_int.rds")
```

#1.2) Records data
Add records data
```{r}
records=read_csv("records_cleaned.csv") %>% 
  mutate(
    h3_id = h3jsr::point_to_cell(
      input = sf::st_as_sf(., coords = c("long", "lat"), crs = 4326),
      res = 3)) %>% 
  #remove marmara and black sea records that are not in the climatic model
  filter(!h3_id%in%c("831ec8fffffffff", "831ec9fffffffff", "832d05fffffffff"))

#Harmonize species
records_harm=bdc::bdc_query_names_taxadb(
  unique(records$species),
  suggest_names = T) %>% 
  select(original_search,suggested_name,notes,scientificName,family) %>% 
  mutate(is_similar=original_search==scientificName)

#replace corrected names
records_harm_corr=records_harm %>% 
  mutate(
    scientificName=case_when(
      original_search%in%"Callionymus filamentosus (Valenciennes, 1837)"~"Callionymus filamentosus",
      original_search%in%"Cryptocentrus steinhardti"~"Cryptocentrus steinhardti",
      original_search%in%"Hippocampus kuda ex H.fuscus"~"Hippocampus kuda",
      original_search%in%"Hippocampus kuda ex H. fuscus"~"Hippocampus kuda",
      original_search%in%"Parupenaeus forskalii"~"Parupeneus forsskali",
      original_search%in%"Equulites popei"~"Equulites popei",                                   original_search%in%"Planiliza carinata"~"Planiliza carinata",
 
      TRUE~scientificName)) %>% 
  filter(!original_search%in%c("Sargocentron sp.",
                               "Sphyraena chrysotaenia /  flavicauda"))

#write.csv(records_harm_corr,"records_harm_corr.csv",row.names = F)
#Fix the records data
records_fix=records %>% 
  left_join(records_harm_corr %>% 
              rename(species=original_search,
                     species_corr=scientificName) %>% 
              select(species,species_corr),
            by="species") %>% 
  select(-species) %>% 
  rename(species=species_corr)

#write.csv(records_fix,"records_cleaned_harm.csv",row.names = F)

#For every h3_id, year,lat, and long keep only the first record
records_min=records_fix %>% 
  group_by(h3_id,species) %>% 
  filter(year%in%min(year)) %>% 
 #For any remaining groups (h3_id/species) with multiple records in the minimum year, keep only the first one
  slice(1) %>%
#Ungroup the data (good practice after grouping operations)
  ungroup()
#Sum annual and spatial records
records_min_count=records_min %>% 
  group_by(h3_id,year) %>% 
  summarise(records=n()) %>% 
  rename(year_of_records=year)

#records_sp_level
records_min_sp_level=records_fix %>% 
  group_by(h3_id,species) %>% 
  filter(year%in%min(year)) %>% 
 #For any remaining groups (h3_id/species) with multiple records in the minimum year, keep only the first one
  slice(1) %>%
#Ungroup the data (good practice after grouping operations)
  ungroup() %>% 
  group_by(h3_id,species,year) %>% 
  summarise(records=n()) %>% 
  rename(year_of_records=year)

```

##1.2.1) Plot records
Plot coords
```{r}
# Assuming your dataframe is named 'records'
# Ensure 'lat' and 'long' columns exist in 'records'

# 1. Get world map data
world_map <- rnaturalearth::ne_countries(scale = "medium", returnclass = "sf")

# 2. Convert your 'records' data frame to an sf object
records_sf <- records_min %>%
  drop_na(lat, long) %>% # Remove rows with missing lat/long
  sf::st_as_sf(coords = c("long", "lat"), crs = 4326) # CRS 4326 is WGS84 for lat/long

# 3. Calculate the extent of your coordinates
bbox <- sf::st_bbox(records_sf)

# 4. Create the ggplot
ggplot() +
  # Add the world map
  geom_sf(data = world_map, fill = "lightgrey", color = "darkgrey") +
  # Add your data points
  geom_sf(data = records_sf, size = 1, alpha = 0.7) +
  # Crop the map to the extent of your points
  coord_sf(
    xlim = c(bbox["xmin"] - 1, bbox["xmax"] + 1), # Adjust buffer as needed
    ylim = c(bbox["ymin"] - 1, bbox["ymax"] + 1), # Adjust buffer as needed
    expand = FALSE # Prevents adding extra space around the crop
  ) +
  # Add labels and theme
  labs(
    title = "Species records",
    x = "Longitude",
    y = "Latitude",
    color = "Species" # Legend title for species color
  ) +
  theme_minimal()+
  guides(color=F)
```

#1.3) Bind MHWs and records
Bind MHWs with the records of the succeeding year
```{r}
used_cells=read_rds("copernicus_data_by_hexagon/MEDSEA_MULTIYEAR_PHY_006_004_10m/corrected_hex_level_data/final_data/all_hexagons_distinct_cells_count.rds") %>% 
    mutate(h3_id = str_remove(h3_id, "_cleaned")) %>% 
  select(h3_id,n_distinct_cells)


c_int_records=annual_c_int %>% 
    filter(h3_id%in%records_min_count$h3_id %>% unique) %>% 
  left_join(records_min_count,
            by=c("h3_id","year_of_records")) %>% 
  mutate(records = replace_na(records, 0)) %>% 
  left_join(used_cells,by="h3_id") %>% 
  mutate(weights=log(n_distinct_cells))
#write.csv(c_int_records,"c_int_records_pseudo_first_fbl.csv",row.names = F)
```

Bind species level
```{r}
# 1. Ungroup the MHW dataset first to avoid data structure errors
annual_c_int_ungrouped <- annual_c_int %>% 
  ungroup()

# Get all unique species per h3_id from the records data
h3_species_combo <- records_min_sp_level %>%
  select(h3_id, species) %>%
  unique()

# Get the h3_id-year combinations along with annual_c_int from the UNGROUPED MHW data
h3_year_combo_mhw <- annual_c_int_ungrouped %>%
  select(h3_id, year_of_mhws, annual_c_int, year_of_records) %>%
  unique()

# 2. Perform the joins to create the final dataset
final_data_clean <- h3_year_combo_mhw %>%
  
  # Cross-join MHW data with all species associated with that h3_id
  # This creates every combination of (h3_id, year, species)
  left_join(h3_species_combo, by = "h3_id") %>%
  
  # Join with the records data to get the 'records' count (1 or NA)
  left_join(records_min_sp_level %>% select(h3_id, species, year_of_records, records), 
            by = c("h3_id", "species", "year_of_records")) %>%
  
  # Replace NA in records with 0
  mutate(records = replace_na(records, 0)) %>%
  
  # Ensure only the necessary columns remain and ordering is correct
  select(h3_id, species, year_of_mhws, annual_c_int, year_of_records, records) %>%
  
  # Ungroup again just to ensure the final output is a regular data frame
  ungroup()

######################################################################
#Now keep only mhw data preexisting to the species record in h3_id
filtered_data_before_first_record <- final_data_clean %>%
  # 1. Group by the unique identifier pairs
  group_by(h3_id, species) %>%
  
  # 2. Arrange the data chronologically (important for finding the 'first' record)
  arrange(year_of_records, .by_group = TRUE) %>%
  
  # 3. Determine the year of the first record (records = 1)
  #    - If records exist (sum(records) > 0), find the earliest year where records == 1.
  #    - If no record exists for this h3_id/species, use the maximum year in the data
  #      to ensure all rows are kept (as there is no 'first record' to stop at).
  mutate(
    first_record_year = if_else(
      sum(records) > 0,
      min(year_of_records[records == 1]),
      max(year_of_records) # Keep all years if the species was never recorded
    )
  ) %>%
  
  # 4. Filter: keep rows where the year is less than or equal to the year of the
  #    first record.
  filter(year_of_records <= first_record_year) %>%
  
  # 5. Remove the helper column and ungroup
  select(-first_record_year) %>%
  ungroup() %>%
  filter(!is.na(species))

#write_rds(filtered_data_before_first_record,"filtered_data_before_first_record.rds")

```

Explore
```{r}
ggplot(c_int_records)+
  geom_point(aes(x=year_of_records,y=records))

ggplot(c_int_records)+
  geom_point(aes(x=annual_c_int,y=records))

ggplot(c_int_records)+
  geom_point(aes(x=year_of_records,y=annual_c_int))
```

#1.4) GLMMs
##aggregated
```{r eval=FALSE, include=FALSE}
library(glmmTMB)

#nbinom2
test2=glmmTMB(data = c_int_records,formula = records~annual_c_int+(1|h3_id),family = "nbinom2",weights = weights)
summary(test2)
performance::check_singularity(test2)
performance::check_convergence(test2)
sjPlot::plot_model(test2,type="eff",terms="annual_c_int",show.data = F)+
  theme_bw()
plot(DHARMa::simulateResiduals(test2))
MuMIn::r.squaredGLMM(test2)
AIC(test2)

#nbinom
test3=glmmTMB(data = c_int_records,formula = records~annual_c_int+(1|h3_id),family = "nbinom1")
summary(test3)
sjPlot::plot_model(test3,type="eff",terms="annual_c_int",show.data = F)
plot(DHARMa::simulateResiduals(test3))
MuMIn::r.squaredGLMM(test3)
AIC(test3)
```

Extract predicted of the best model
```{r}
test2_gg=ggeffects::ggpredict(test2,terms="annual_c_int[0:381 by=1]")
```

Plot
```{r}
# 1. Extract the p-value for 'annual_c_int'
# The p-value is in the fixed effects part of the model summary.
p_value <- summary(test2)$coefficients$cond["annual_c_int", "Pr(>|z|)"]

# 2. Extract the sample size (number of observations)
N <- nobs(test2)

# 3. Create a formatted label string
# Format the p-value to a specific number of decimal places (e.g., 3)
p_label <- paste0("p-value ", format.pval(p_value, digits = 3, eps = 0.001))
N_label <- paste0("n = ", N)

# 4. Determine the position for the labels
# These coordinates are illustrative; adjust them to fit your plot data.
# Assuming x is from 0 to about 120 and y is on a log scale (say max 1000).
x_pos <- max(c_int_records_dt$annual_c_int, na.rm = TRUE) * 0.95 # Near top-right
y_pos_p <- max(c_int_records_dt$records, na.rm = TRUE) * 0.95 
y_pos_N <- max(c_int_records_dt$records, na.rm = TRUE) * 0.7 # Adjust vertical separation on log scale

# Convert y_pos to a single data frame for geom_text/annotate if preferred
stats_df <- data.frame(
  x = c(x_pos, x_pos),
  y = c(y_pos_p, y_pos_N),
  label = c(p_label, N_label)
)

#plot
test2_p=ggplot()+
  geom_point(data=c_int_records,
             aes(x=annual_c_int,y=records),
             color="grey",
             alpha=0.5)+
  geom_ribbon(data=test2_gg,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3)+
  geom_line(data=test2_gg,
            aes(x=x,y=predicted))+
  scale_fill_gradient(low = "grey70", high = "red", guide = "none") +
  theme_bw()+
  #scale_y_log10()+
  labs(x="Annual cumulative intensity (°C-days)",
       y="New records within the analysis period") +
  #coord_cartesian(ylim=c(0,3))+
  # Add the labels using annotate() for fixed positions
  annotate("text", x = 381, y = 16, 
           label = p_label, 
           hjust = 1, vjust = 1, size = 4) +
  annotate("text", x = 381, y = 14, 
           label = N_label, 
           hjust = 1, vjust = 1, size = 4)+
  ggbreak::scale_y_break(breaks=c(2,3),scales = c(2,0.5))+
  scale_y_continuous(breaks=c(0,1,2,5,10,15,20))
test2_p
ggsave(filename ="test2_p.png" ,plot =test2_p ,device = "jpeg",units = "cm",width = 15,height = 10)
```

##species_level
```{r}
library(glmmTMB)

test_sp=glmmTMB(data = filtered_data_before_first_record,
              formula = records~annual_c_int+(1|h3_id)+(1|species),
              family = "binomial")
summary(test_sp)
performance::check_singularity(test_sp)
performance::check_convergence(test_sp)
sjPlot::plot_model(test_sp,type="eff",terms="annual_c_int",show.data = T)+
  theme_bw()
plot(DHARMa::simulateResiduals(test_sp))
MuMIn::r.squaredGLMM(test_sp)
AIC(test_sp)

test_sp_gg=ggeffects::ggpredict(test_sp,terms="annual_c_int [0:240 by=1]")
```

Plot species level
```{r}
# 1. Extract the p-value for 'annual_c_int'
# The p-value is in the fixed effects part of the model summary.
p_value <- summary(test_sp)$coefficients$cond["annual_c_int", "Pr(>|z|)"]

# 2. Extract the sample size (number of observations)
N <- nobs(test_sp)

# 3. Create a formatted label string
# Format the p-value to a specific number of decimal places (e.g., 3)
p_label <- paste0("p-value ", format.pval(p_value, digits = 5, eps = 0.0001))
N_label <- paste0("n = ", N)

# 4. Determine the position for the labels
# These coordinates are illustrative; adjust them to fit your plot data.
# Assuming x is from 0 to about 120 and y is on a log scale (say max 1000).
x_pos <- max(filtered_data_before_first_record$annual_c_int, na.rm = TRUE) * 0.95 # Near top-right
y_pos_p <- max(filtered_data_before_first_record$records, na.rm = TRUE) * 0.95 
y_pos_N <- max(filtered_data_before_first_record$records, na.rm = TRUE) * 0.7 # Adjust vertical separation on log scale

# Convert y_pos to a single data frame for geom_text/annotate if preferred
stats_df <- data.frame(
  x = c(x_pos, x_pos),
  y = c(y_pos_p, y_pos_N),
  label = c(p_label, N_label)
)

test_sp_p=ggplot(filtered_data_before_first_record)+
  geom_point(aes(x=annual_c_int,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_gg,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_gg,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Annual cumulative intensity (°C-days)",
       y="Record status",
       title="A) Fixed baseline")+
  theme_bw()+
  annotate("text", x = 55, y = 0.9, 
           label = p_label, 
           hjust = 1, vjust = 1, size = 4)# +
  # annotate("text", x = 40, y = 0.8, 
  #          label = N_label, 
  #          hjust = 1, vjust = 1, size = 4)
test_sp_p
ggsave(filename ="test_sp_p.png" ,plot =test_sp_p ,device = "jpeg",units = "cm",width = 15,height = 10)
```


#2.1) detrended cumulative intensity
Estimate annual cumulative intensity per hexagon
```{r}
annual_c_int_dt=clim_dt %>% 
  drop_na(residuals) %>% 
  mutate(
    # Create a new column, e.g., 'intensity'
    intensity = dplyr::if_else(
      !is.na(event_no), # The condition: is event_no not NA?
      residuals - seas, # What to do if the condition is TRUE
      0 # What to do if the condition is FALSE
    )) %>% 
    mutate(year_of_mhws=lubridate::year(date)) %>% 
  group_by(h3_id,year_of_mhws) %>% 
  summarise(annual_c_int=sum(intensity)) %>% 
  mutate(year_of_records=year_of_mhws+1)

#write_rds(annual_c_int_dt,"annual_c_int_dt.rds")
```

#2.2) Bind MHWs and records
Bind MHWs with the records of the succeeding year
```{r}
#add used cells per hexagon
used_cells=read_rds("copernicus_data_by_hexagon/MEDSEA_MULTIYEAR_PHY_006_004_10m/corrected_hex_level_data/final_data/all_hexagons_distinct_cells_count.rds") %>% 
    mutate(h3_id = str_remove(h3_id, "_cleaned")) %>% 
  select(h3_id,n_distinct_cells)

c_int_records_dt=annual_c_int_dt %>% 
  filter(h3_id%in%records_min_count$h3_id %>% unique) %>% 
  left_join(records_min_count,
            by=c("h3_id","year_of_records")) %>% 
  mutate(records = replace_na(records, 0)) %>% 
  left_join(used_cells,by="h3_id") %>% 
  mutate(weights=log(n_distinct_cells))
#write.csv(c_int_records_dt,"c_int_records_pseudo_first_dt.csv",row.names = F)
```

Explore
```{r}
ggplot(c_int_records_dt)+
  geom_point(aes(x=year_of_records,y=records))

ggplot(c_int_records_dt)+
  geom_point(aes(x=annual_c_int,y=records))

ggplot(c_int_records_dt)+
  geom_point(aes(x=year_of_records,y=annual_c_int))
```

Bind species level
```{r}
# 1. Ungroup the MHW dataset first to avoid data structure errors
annual_c_int_ungrouped_dt <- annual_c_int_dt %>% 
  ungroup()

# Get all unique species per h3_id from the records data
h3_species_combo_dt <- records_min_sp_level %>%
  select(h3_id, species) %>%
  unique()

# Get the h3_id-year combinations along with annual_c_int_dt from the UNGROUPED MHW data
h3_year_combo_mhw_dt <- annual_c_int_ungrouped_dt %>%
  select(h3_id, year_of_mhws, annual_c_int, year_of_records) %>%
  unique()

# 2. Perform the joins to create the final dataset
final_data_clean_dt <- h3_year_combo_mhw_dt %>%
  
  # Cross-join MHW data with all species associated with that h3_id
  # This creates every combination of (h3_id, year, species)
  left_join(h3_species_combo_dt, by = "h3_id") %>%
  
  # Join with the records data to get the 'records' count (1 or NA)
  left_join(records_min_sp_level %>% select(h3_id, species, year_of_records, records), 
            by = c("h3_id", "species", "year_of_records")) %>%
  
  # Replace NA in records with 0
  mutate(records = replace_na(records, 0)) %>%
  
  # Ensure only the necessary columns remain and ordering is correct
  select(h3_id, species, year_of_mhws, annual_c_int, year_of_records, records) %>%
  
  # Ungroup again just to ensure the final output is a regular data frame
  ungroup()

######################################################################
#Now keep only mhw data preexisting to the species record in h3_id
filtered_data_before_first_record_dt <- final_data_clean_dt %>%
  # 1. Group by the unique identifier pairs
  group_by(h3_id, species) %>%
  
  # 2. Arrange the data chronologically (important for finding the 'first' record)
  arrange(year_of_records, .by_group = TRUE) %>%
  
  # 3. Determine the year of the first record (records = 1)
  #    - If records exist (sum(records) > 0), find the earliest year where records == 1.
  #    - If no record exists for this h3_id/species, use the maximum year in the data
  #      to ensure all rows are kept (as there is no 'first record' to stop at).
  mutate(
    first_record_year = if_else(
      sum(records) > 0,
      min(year_of_records[records == 1]),
      max(year_of_records) # Keep all years if the species was never recorded
    )
  ) %>%
  
  # 4. Filter: keep rows where the year is less than or equal to the year of the
  #    first record.
  filter(year_of_records <= first_record_year) %>%
  
  # 5. Remove the helper column and ungroup
  select(-first_record_year) %>%
  ungroup() %>%
  filter(!is.na(species)) #%>% 
  # left_join(read_rds(
  # "copernicus_data_by_hexagon/MEDSEA_MULTIYEAR_PHY_006_004_10m//corrected_hex_level_data/final_data/all_hexagons_daily_summary_10m.rds") %>%
  # mutate(h3_id = str_remove(h3_id, "_cleaned"),
  #        year_of_mhws=lubridate::year(date)) %>% 
  # group_by(h3_id,year_of_mhws) %>% 
  #            summarise(median_sst=median(mean_sst_10m)),by=c("h3_id","year_of_mhws")) %>% 
  # left_join(ormef_h3_id,by="h3_id") %>% 
  # mutate(weights=1/sources)

#write_rds(filtered_data_before_first_record_dt,"filtered_data_before_first_record_dt.rds")

```


#2.4) GLMMs
###monitoring effort control
Account for monitoring effort by years and h3_id
This specifically accounts for the increased monitoring effort over time

Use only hexagons that are within both datasets.
```{r}
filtered_data_before_first_record_dt_sst=read_rds("filtered_data_before_first_record_dt_sst.rds")
h3_only_records=intersect(filtered_data_before_first_record_dt_sst$h3_id %>% unique(),ormef_h3_id_year$h3_id %>% unique)

filtered_data_before_first_record_dt_sst_meff=
  filtered_data_before_first_record_dt_sst %>%
  filter(h3_id%in%h3_only_records) %>% 
  select(-sources,-weights) %>% 
  left_join(ormef_h3_id_year %>% 
              rename(year_of_records=Year),
            by=c("h3_id","year_of_records")) %>% 
  mutate(sources=case_when(is.na(sources)~0,
                           TRUE~sources))

source_temp=ggplot(filtered_data_before_first_record_dt_sst_meff %>% 
                     ungroup() %>% 
                     distinct(h3_id,year_of_records,sources))+
  geom_jitter(aes(x=year_of_records,y=sources),height = 0.1)+
  #geom_violin(aes(x=year_of_records,y=sources,group=year_of_records))+
  theme_bw()+
  labs(x="Year",y="Sources",title="B) Temporal trend")
source_temp
ggplot(filtered_data_before_first_record_dt_sst_meff %>% 
         mutate(points=h3jsr::cell_to_point(h3_id)))+
  geom_point(aes(x=year_of_records,y=sources))+
  theme_bw()+
  labs(x="Year",y="Sources",title="A) Temporal trend")

cor.test(filtered_data_before_first_record_dt_sst_meff$sources,filtered_data_before_first_record_dt_sst_meff$year_of_mhws)

#species
filtered_data_before_first_record_dt_sst_meff$species %>% unique
#hexagons
filtered_data_before_first_record_dt_sst_meff$h3_id %>% unique
#Records
filtered_data_before_first_record_dt_sst_meff %>% 
  ungroup() %>% 
  filter(records%in%1) %>% 
  distinct(h3_id,species,year_of_mhws,records)

#write the analysis data with sources
# write_rds(filtered_data_before_first_record_dt_sst_meff,"filtered_data_before_first_record_dt_sst_meff.rds")
```

bias grid
```{r}
bias_grid=gridExtra::grid.arrange(
  h3_map,
                        source_temp,
  layout_matrix = rbind(c(1, 1,1,1,1,1,1),
                        c(1, 1,1,1,1,1,1),
                        c(1, 1,1,1,1,1,1),# Row 1: P1 and P2
                        c(NA,2,2,2,2,2,NA),
                        c(NA,2,2,2,2,2,NA)))
ggsave(filename ="bias_grid.png" ,plot =bias_grid ,device = "jpeg",units = "cm",width = 20,height = 20)
```

###Test
Load the data
```{r}
filtered_data_before_first_record_dt_sst_meff=read_rds("filtered_data_before_first_record_dt_sst_meff.rds") %>%#add the DCI_p index 
  left_join(read.csv("dci_index.csv") %>% 
              select(h3_id,
                     DCI_P),
            by="h3_id") %>% 
  left_join(read.csv("cpi_data_with_C_all.csv") %>% 
              select(h3_id,
                     CPI),
            by = join_by(h3_id))

year_of_f_rec=ormef %>% 
  group_by(Species) %>% 
  summarise(first_rec_year=min(Year)) %>% 
  rename(species=Species) %>% 
  mutate(species=case_when(species%in%"Torquigener flavimaculosus"~"Torquigener hypselogeneion",
            species%in%"Chelon carinatus"~"Planiliza carinata",
            species%in%"Oxyurichthys petersi"~"Oxyurichthys petersii",
            species%in%"Chelon carinatus"~"Cryptocentrus caeruleopunctatus",
            species%in%"Hippocampus fuscus"~"Hippocampus kuda",
            TRUE~species))

filtered_data_before_first_record_dt_sst_meff=filtered_data_before_first_record_dt_sst_meff %>% 
  left_join(year_of_f_rec,
            by="species") %>% 
  mutate(first_rec_year=case_when(species%in%"Cryptocentrus caeruleopunctatus"~2014,
                                  species%in%"Himantura leoparda"~2016,#consider 2008 - paper
                                  species%in%"Sardinella gibbosa"~2008,
                                  species%in%"Ablennes hians"~2018,
                                  species%in%"Cryptocentrus steinhardti"~2012,
                                  species%in%"Sargocentron caudimaculatum"~2021,
                                  TRUE~first_rec_year))

species_yfr=filtered_data_before_first_record_dt_sst_meff %>% 
  select(species,first_rec_year) %>% 
  distinct(species,first_rec_year)

#write.csv(species_yfr,"species_yfr.csv",row.names = F)
```

I am going to add current-weighted distance to the source variable.
```{r}
test_sp_dt_time_sst_mon_eff=glmmTMB(
  data = filtered_data_before_first_record_dt_sst_meff,
  formula = records~
    scale(first_rec_year)+
    scale(CPI)+
    scale(sources)+
    scale(annual_c_int)+
    scale(year_of_mhws)+
    scale(median_sst)+
    (1|h3_id)+
    (1|species),
  family = "betabinomial",
  control = glmmTMBControl(optimizer = optim,
                           optArgs = list(method = "BFGS",
                                          maxit = 10000)))
#write_rds(test_sp_dt_time_sst_mon_eff,"test_sp_dt_time_sst_mon_eff_cpi.rds")
test_sp_dt_time_sst_mon_eff=read_rds("glmms\\test_sp_dt_time_sst_mon_eff_cpi/test_sp_dt_time_sst_mon_eff_cpi.rds")
summary(test_sp_dt_time_sst_mon_eff)
plot(DHARMa::simulateResiduals(test_sp_dt_time_sst_mon_eff))
performance::check_singularity(test_sp_dt_time_sst_mon_eff)
performance::check_convergence(test_sp_dt_time_sst_mon_eff)
performance::check_collinearity(test_sp_dt_time_sst_mon_eff)
MuMIn::r.squaredGLMM(test_sp_dt_time_sst_mon_eff)

AIC(test_sp_dt_time_sst_mon_eff)

#AIC with DPIp - 7689.4
#AIC with CPI - 7689.608

#Summarise table for the model
fixed_effects_df <- broom.mixed::tidy(test_sp_dt_time_sst_mon_eff, effects = "fixed") %>%
  # Select and rename columns to match the request
  select(
    Variable = term,
    Coefficient = estimate,
    `Std. Error` = std.error,
    `Z Value` = statistic,
    `P Value` = p.value
  ) %>%
  # Format numeric columns for clean presentation (e.g., 3 decimal places)
  mutate(
    Coefficient = round(Coefficient, 3),
    `Std. Error` = round(`Std. Error`, 3),
    `Z Value` = round(`Z Value`, 2),
    `P Value` = format.pval(`P Value`, digits = 3, eps = 0.0001)
  )
#random effects
random_effects_df <- broom.mixed::tidy(test_sp_dt_time_sst_mon_eff, effects = "ran_pars") 

#write.csv(fixed_effects_df,"fixed_effects_df.csv",row.names = F)

#Binomial model: AIC = 7687.6
#Betabinomial model: AIC = 7689.6
#Delta is 2 - models are quite similar - I am chossing base on r^2
```

Extract predictions
```{r}
test_sp_dt_time_sst_mon_eff_gg_cint=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("annual_c_int [0:183 by =1]"))

test_sp_dt_time_sst_mon_eff_gg_yr=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("year_of_mhws [1987:2022 by =1]"))

test_sp_dt_time_sst_mon_eff_gg_sst=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("median_sst [14:24 by = 0.5]"))

test_sp_dt_time_sst_mon_eff_gg_source=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("sources [0:6 by = 1]"))

test_sp_dt_time_sst_mon_eff_gg_cpi=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("CPI [-0.09:0.09 by = 0.001]"))

test_sp_dt_time_sst_mon_eff_gg_yfr=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("first_rec_year [1924:2021 by = 1]"))

write_rds(test_sp_dt_time_sst_mon_eff_gg_cint,
          "test_sp_dt_time_sst_mon_eff_gg_cint.rds")
write_rds(test_sp_dt_time_sst_mon_eff_gg_yr,
          "test_sp_dt_time_sst_mon_eff_gg_yr.rds")
write_rds(test_sp_dt_time_sst_mon_eff_gg_sst,
          "test_sp_dt_time_sst_mon_eff_gg_sst.rds")
write_rds(test_sp_dt_time_sst_mon_eff_gg_source,
          "test_sp_dt_time_sst_mon_eff_gg_source.rds")
write_rds(test_sp_dt_time_sst_mon_eff_gg_cpi,
          "test_sp_dt_time_sst_mon_eff_gg_cpi.rds")
write_rds(test_sp_dt_time_sst_mon_eff_gg_yfr,
          "test_sp_dt_time_sst_mon_eff_gg_yfr.rds")
```

####Plot model
```{r}
#c_intensity
test_sp_dt_time_sst_mon_eff_cint_p=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_point(aes(x=annual_c_int,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_cint,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_cint,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Cumulative intensity (°C-days)",
       y="Record status",
       title="C) MHWs")+
  theme_bw()+
  theme(axis.title.y = element_blank())


#c_intensity zoom
test_sp_dt_time_sst_mon_eff_cint_p_zoom=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_cint,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_cint,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Cumulative intensity (°C-days)",
       y="Record status")+
  theme_bw()+
  theme(axis.title.y = element_blank())+
coord_cartesian(ylim=c(0,.1))+
  scale_y_continuous(breaks = seq(0,0.1,by=0.05))


#embed cint
embeded_cint <- test_sp_dt_time_sst_mon_eff_cint_p +
  annotation_custom(ggplotGrob(test_sp_dt_time_sst_mon_eff_cint_p_zoom),
                    xmin = 50, xmax = 150,
                    ymin = 0.1, ymax = 0.9)


#years
test_sp_dt_time_sst_mon_eff_yr_p=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_point(aes(x=year_of_mhws,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_yr,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_yr,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Year",
       y="Record status",
       title="A) Time")+
  theme_bw()+
  theme(axis.title.y = element_blank())


#SST
test_sp_dt_time_sst_mon_eff_sst_p=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_point(aes(x=median_sst,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_sst,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_sst,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="SST (°C)",
       y="Record status",
       title="B) SST")+
  theme_bw()+
  theme(axis.title.y = element_blank())

#SST zoom
test_sp_dt_time_sst_mon_eff_sst_p_zoom=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_sst,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_sst,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="SST (°C)",
       y="Record status")+
  theme_bw()+
  coord_cartesian(ylim=c(0,.1))+
  scale_y_continuous(breaks = seq(0,0.1,by=0.05))+
  theme(axis.title = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA))

#embed sst
embeded_sst <- test_sp_dt_time_sst_mon_eff_sst_p +
  annotation_custom(ggplotGrob(test_sp_dt_time_sst_mon_eff_sst_p_zoom),
                    xmin = 15, xmax = 23,
                    ymin = 0.1, ymax = 0.9)+
  theme(axis.title.y = element_blank())

```

```{r}
test_sp_dt_time_sst_mon_eff_grid=gridExtra::grid.arrange(left="Probability of record occurrence",
  test_sp_dt_time_sst_mon_eff_yr_p,
                        embeded_sst,
                        test_sp_dt_time_sst_mon_eff_cint_p,
                        ncol = 2,
                        nrow=2,
  layout_matrix = rbind(c(1, 1,1,1), # Row 1: P1 and P2
                        c(2,2,3,3)))


ggsave(filename ="test_sp_dt_time_sst_mon_eff_grid.png" ,plot =test_sp_dt_time_sst_mon_eff_grid ,device = "jpeg",units = "cm",width = 20,height = 15)
```

Plot non-focal
```{r}
#Source
test_sp_dt_time_sst_mon_eff_source_p=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_point(aes(x=sources,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_source,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_source,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Sources",
       y="Record status",
       title="A) Sources")+
  theme_bw()+
  theme(axis.title.y = element_blank())

#CPI
test_sp_dt_time_sst_mon_eff_cpi_p=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_point(aes(x=CPI,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_cpi,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_cpi,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="CPI (m/s)",
       y="Probability of record occurrence",
       title="B) Current Propagation Index")+
  theme_bw()+
  theme(axis.title.y = element_blank())
test_sp_dt_time_sst_mon_eff_cpi_p

#Year of first record
test_sp_dt_time_sst_mon_eff_yfr_p=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_point(aes(x=first_rec_year,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_yfr,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_yfr,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Year of first record",
       y="Probability of record occurrence",
       title="C) Year of first record")+
  theme_bw()+
  theme(axis.title.y = element_blank())+
    scale_x_continuous(breaks = seq(1920,2020,by=20))
test_sp_dt_time_sst_mon_eff_yfr_p

#YFR zoom
test_sp_dt_time_sst_mon_eff_yfr_p_zoom=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_yfr,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_yfr,
              aes(x=x,y=predicted),
            linewidth=1)+
  theme_bw()+
  coord_cartesian(ylim=c(0,.08))+
  scale_y_continuous(breaks = seq(0,0.08,by=0.04))+
  scale_x_continuous(breaks = seq(1920,2020,by=20))+
  theme(axis.title = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA))

#embed yfr
embeded_yfr <- test_sp_dt_time_sst_mon_eff_yfr_p +
  annotation_custom(ggplotGrob(test_sp_dt_time_sst_mon_eff_yfr_p_zoom),
                    xmin = 1940, xmax = 2000,
                    ymin = 0.1, ymax = 0.9)+
  theme(axis.title.y = element_blank())
```

Non climatic panel
```{r}
test_sp_dt_time_sst_mon_eff_grid_nc=gridExtra::grid.arrange(
  left="Probability of record occurrence",
  test_sp_dt_time_sst_mon_eff_source_p,
  test_sp_dt_time_sst_mon_eff_cpi_p,
  embeded_yfr,
  layout_matrix = rbind(c(1), # Row 1: P1 and P2
                        c(2),
                        c(3)))


ggsave(filename ="test_sp_dt_time_sst_mon_eff_grid_nc.png" ,plot =test_sp_dt_time_sst_mon_eff_grid_nc ,device = "jpeg",units = "cm",width = 15,height = 20)
```

###Plot used records
```{r}
# 2. Convert H3 IDs to Spatial Polygons (sf object)
used_records_sf <- filtered_data_before_first_record_dt_sst_meff %>%
  group_by(h3_id) %>%
  summarise(sum_records=sum(records)) %>% 
  # Pass the entire data frame and specify the H3 column name
  h3jsr::cell_to_polygon(
    input = .,               # '.' refers to the incoming data frame
    simple = FALSE           # Recommended for a tidy output
  ) %>%
  # The output is now a proper sf object, so st_set_crs works
  sf::st_set_crs(4326)

# 3. Get Mediterranean Land Boundaries for Context
# Function from rnaturalearth
world <-rnaturalearth:: ne_countries(scale = "large", returnclass = "sf")
ocean <-rnaturalearth:: ne_download(scale = "large",
                                    type = "ocean",
                                    category = "physical")

# 4. Create the ggplot map
records_map <- ggplot() +
    # Plot the H3 Hexagons (your data)
  geom_sf(
    data = used_records_sf,
    # Fills the hexagons based on the 'sources' count
    aes(fill = sum_records), 
    color = "black",     
    linewidth = 0.1,     
    alpha = 0.8          
  ) +
  # Add land boundaries first 
  geom_sf(data = world, fill = "antiquewhite", color = "black", linewidth = 0.1) +
  # Customize the color scale (from ggplot2 / viridis package, which tidyverse uses)
  scale_fill_viridis_c(
    name = "Records",
    option = "turbo"
  ) +
  # Set the map extent to focus on the Mediterranean Sea
  coord_sf(
    xlim = c(-6, 38),   
    ylim = c(26, 48),  
    expand = FALSE
  ) +
  # Apply a clean theme and labels
  labs(
    title = "A) First records within the temoral range",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_bw() +
  theme(
# --- CHANGE IS HERE ---
 # Moves the legend inside the plot using (x, y) coordinates
legend.position = c(0.22, 0.17),
# Optional: Makes the legend background transparent
legend.background = element_rect(fill = "transparent", color = NA),
legend.key.size = unit(0.25, "cm"),
panel.grid.major = element_line(color = "skyblue", linewidth = 0.2),
    panel.background = element_rect(fill = "skyblue", color = NA) 
  )
records_map
ggsave(filename ="records_map.png" ,plot =records_map ,device = "jpeg",units = "cm",width = 25,height = 12)
```

sst over time
```{r}
sst_time=filtered_data_before_first_record_dt_sst_meff %>% 
  group_by(h3_id) %>% 
  distinct(h3_id,year_of_mhws,median_sst)
sst_time_p=ggplot(sst_time)+
  geom_point(aes(x=year_of_mhws,y=median_sst,color=median_sst),
             alpha=0.3)+
  geom_boxplot(aes(x=year_of_mhws,y=median_sst,group=year_of_mhws),
               alpha=0.2,notch=T,outliers = F)+
  geom_smooth(aes(x=year_of_mhws,y=median_sst),
              formula = y~x,method = "lm",se = F,color="black")+
  scale_color_viridis_c(
    name = "SST",
    option = "turbo"
  ) +
  labs(x="Years",y="Median SST (°C)",
       title="B) SST trend")+
  guides(color=F)+
  theme_bw()
sst_time_p
ggsave(filename ="sst_time_p.png" ,plot =sst_time_p ,device = "jpeg",units = "cm",width = 20,height = 12)
```

Cumulative intensity over time
```{r}
c_int_time=filtered_data_before_first_record_dt_sst_meff %>% 
  group_by(h3_id) %>% 
  distinct(h3_id,year_of_mhws,annual_c_int)

c_int_time_p=ggplot(c_int_time)+
  geom_point(aes(x=year_of_mhws,y=annual_c_int,color=annual_c_int),
             alpha=0.3)+
  geom_boxplot(aes(x=year_of_mhws,y=annual_c_int,group=year_of_mhws),
               alpha=0.2,notch=F,outliers = F)+
  scale_color_viridis_c(
    name = "Cumulative intensity",
    option = "turbo"
  ) +
  labs(x="Years",y="Cumulative intensity (°C-days)",
       title="C) MHWs detrended")+
  guides(color=F)+
  theme_bw()#+
  # theme(legend.position=c(0.85, 0.8),
  #       legend.background = element_blank(),
  #       legend.key.size = unit(0.4, 'cm'))
c_int_time_p
ggsave(filename ="c_int_time_p.png" ,plot =c_int_time_p ,device = "jpeg",units = "cm",width = 20,height = 12)
```

Into grid
```{r}
intro_grid=gridExtra::grid.arrange(
  records_map,
  sst_time_p,
  c_int_time_p,
  ncol = 2,
  nrow=2,
  layout_matrix = rbind(c(1,1,1,1,1,1), # Row 1: P1 and P2
                        c(1,1,1,1,1,1),
                        c(1,1,1,1,1,1),
                        c(2,2,2,3,3,3),
                        c(2,2,2,3,3,3)))

intro_grid
ggsave(filename ="intro_grid.png" ,plot =intro_grid ,device = "jpeg",units = "cm",width = 20,height = 20)
```


##dredge
```{r}
# Dredge searches all possible combinations of fixed effects.
# It automatically keeps the random effects ((1|h3_id) + (1|species)) in ALL models.

# 1.1 Set MuMIn's options to prevent NA's from failing the model
# This is crucial for dredge()
options(na.action = "na.fail")

# It also generates the null model (intercept only) and the global model (all fixed effects).
cat("\nPerforming Model Dredging (comparing all 64 models)...\n")
model_selection_table <-MuMIn::dredge(
  test_sp_dt_time_sst_mon_eff,
  rank = "AICc" # Use AIC corrected for small sample sizes
)

write_rds(model_selection_table,"model_selection_table.rds")
write.csv(model_selection_table %>% as_tibble(),"model_selection_table.csv",row.names = F)

# ==============================================================================
# STEP 3: MANUAL FILTERING AND AIC WEIGHT CALCULATION
# ==============================================================================
cat("\n-- Filtering and Recalculating Weights for Converged Models --\n")

# 3.1 Filter out all models that failed to converge (where AICc is NA)
valid_models <- subset(model_selection_table, !is.na(AICc))

# 3.2 Recalculate Delta AICc based ONLY on the converged models' best score
# This ensures delta=0 corresponds to the best CONVERGED model
min_AICc <- min(valid_models$AICc)
valid_models$delta <- valid_models$AICc - min_AICc

valid_models=valid_models %>% 
  as.data.frame()
valid_models$weight=as.numeric(valid_models$weight)
# 3.3 Manually Calculate Akaike Weights (Formula: exp(-0.5 * delta) / sum(exp(-0.5 * delta)))
# This is equivalent to MuMIn's weight() function, applied to the subset
valid_models=valid_models %>% 
  mutate(weight=exp(-0.5 * valid_models$delta) / sum(exp(-0.5 * valid_models$delta))) %>%
    mutate(across(where(is.numeric), ~ round(.x, 3)))

# Sort the valid models by the newly calculated AICc rank
valid_models <- valid_models[order(valid_models$AICc), ]
print(failed_models)

#write.csv(valid_models,"valid_selected_models.csv",row.names = F)

# ==============================================================================
# STEP 4: OUTPUT RESULTS
# ==============================================================================

cat("\n=======================================================\n")
cat(paste0(" MODEL SELECTION TABLE (N=", nrow(valid_models), " Converged Models) "))
cat("\n=======================================================\n")

# Print the top 10 *converged* models
print(head(valid_models, n = 10))

cat("\nInterpretation:\n")
cat(" - The AICc, delta, and weight columns are now only calculated for the 46 converged models.\n")
cat(" - delta: The difference in AICc between a model and the best CONVERGED model (delta=0).\n")
cat(" - weight: The Akaike weight, showing the probability of being the best model in this set.\n")

# Calculate variable importance (Sum of weights for each predictor)
# We must use the original sw() function on the VALID models
importance_ranking <- MuMIn::sw(valid_models)
cat("\n=======================================================\n")
cat("             RELATIVE VARIABLE IMPORTANCE              \n")
cat("=======================================================\n")
# Importance close to 1 indicates the variable is in all or most top-ranking models.
print(importance_ranking)

# Reset global options
options(na.action = "na.omit")
```

###test with DCIp
```{r}
test_sp_dt_time_sst_mon_eff_dcip=glmmTMB(
  data = filtered_data_before_first_record_dt_sst_meff,
  formula = records~
    scale(first_rec_year)+
    scale(DCI_P)+
    scale(sources)+
    scale(annual_c_int)+
    scale(year_of_mhws)+
    scale(median_sst)+
    (1|h3_id)+
    (1|species),
  family = "betabinomial")
test_sp_dt_time_sst_mon_eff_dcip %>% summary
```

####dredge - dcip
```{r}

# Dredge searches all possible combinations of fixed effects.
# It automatically keeps the random effects ((1|h3_id) + (1|species)) in ALL models.

# 1.1 Set MuMIn's options to prevent NA's from failing the model
# This is crucial for dredge()
options(na.action = "na.fail")

# It also generates the null model (intercept only) and the global model (all fixed effects).
cat("\nPerforming Model Dredging (comparing all 64 models)...\n")
model_selection_table_dcip <-MuMIn::dredge(
  test_sp_dt_time_sst_mon_eff_dcip,
  rank = "AICc" # Use AIC corrected for small sample sizes
)
model_selection_table_dcip
write_rds(model_selection_table_dcip,"model_selection_table_dcip.rds")
write.csv(model_selection_table_dcip %>% as_tibble(),"model_selection_table_dcip.csv",row.names = F)

# ==============================================================================
# STEP 3: MANUAL FILTERING AND AIC WEIGHT CALCULATION
# ==============================================================================
cat("\n-- Filtering and Recalculating Weights for Converged Models --\n")

# 3.1 Filter out all models that failed to converge (where AICc is NA)
valid_models_dcip <- subset(model_selection_table_dcip, !is.na(AICc))

# 3.2 Recalculate Delta AICc based ONLY on the converged models' best score
# This ensures delta=0 corresponds to the best CONVERGED model
min_AICc <- min(valid_models_dcip$AICc)
valid_models_dcip$delta <- valid_models_dcip$AICc - min_AICc

valid_models_dcip=valid_models_dcip %>% 
  as.data.frame()
valid_models_dcip$weight=as.numeric(valid_models_dcip$weight)
# 3.3 Manually Calculate Akaike Weights (Formula: exp(-0.5 * delta) / sum(exp(-0.5 * delta)))
# This is equivalent to MuMIn's weight() function, applied to the subset
valid_models_dcip=valid_models_dcip %>% 
  mutate(weight=exp(-0.5 * valid_models_dcip$delta) / sum(exp(-0.5 * valid_models_dcip$delta))) %>%
    mutate(across(where(is.numeric), ~ round(.x, 3)))

# Sort the valid models by the newly calculated AICc rank
valid_models_dcip <- valid_models_dcip[order(valid_models_dcip$AICc), ]
print(failed_models)

#write.csv(valid_models_dcip,"valid_selected_models_dcip.csv",row.names = F)

# ==============================================================================
# STEP 4: OUTPUT RESULTS
# ==============================================================================

cat("\n=======================================================\n")
cat(paste0(" MODEL SELECTION TABLE (N=", nrow(valid_models), " Converged Models) "))
cat("\n=======================================================\n")

# Print the top 10 *converged* models
print(head(valid_models_dcip, n = 10))

cat("\nInterpretation:\n")
cat(" - The AICc, delta, and weight columns are now only calculated for the 46 converged models.\n")
cat(" - delta: The difference in AICc between a model and the best CONVERGED model (delta=0).\n")
cat(" - weight: The Akaike weight, showing the probability of being the best model in this set.\n")

# Calculate variable importance (Sum of weights for each predictor)
# We must use the original sw() function on the VALID models
importance_ranking <- MuMIn::sw(valid_models_dcip)
cat("\n=======================================================\n")
cat("             RELATIVE VARIABLE IMPORTANCE              \n")
cat("=======================================================\n")
# Importance close to 1 indicates the variable is in all or most top-ranking models.
print(importance_ranking)

# Reset global options
options(na.action = "na.omit")
```

