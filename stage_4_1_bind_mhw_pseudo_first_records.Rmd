---
title: "stage_4_bind_mhw_records"
author: "Shahar Chaikin"
date: "2025-09-24"
output: html_document
---

Bind climatologies with records data

Libraries
```{r}
library(tidyverse)
```

#data
```{r}
#trended
 clim=read_rds("climatology_df_median_sst_10m.rds")
 events=read_rds("event_df_median_sst_10m.rds")
#detrended
clim_dt=read_rds("climatology_dt_df.rds")
events_dt=read_rds("event_dt_df.rds")
```

#1.1) annual cumulative intensity
Estimate annual cumulative intensity per hexagon
```{r eval=FALSE, include=FALSE}
annual_c_int=clim %>% 
  drop_na(median_sst_10m) %>% 
  mutate(
    # Create a new column, e.g., 'temp_anomaly'
    intensity = dplyr::if_else(
      !is.na(event_no), # The condition: is event_no not NA?
      median_sst_10m - seas, # What to do if the condition is TRUE
      0 # What to do if the condition is FALSE
    )) %>% 
    mutate(year_of_mhws=lubridate::year(date)) %>% 
  group_by(h3_id,year_of_mhws) %>% 
  summarise(annual_c_int=sum(intensity)) %>% 
  mutate(year_of_records=year_of_mhws+1)

#write_rds(annual_c_int,"annual_c_int.rds")
```

#1.2) Records data
Add records data
```{r}
records=read_csv("records data/cleaned/records_cleaned_harm_8_10_25.csv") %>% 
  #remove marmara and black sea records that are not in the climatic model
  filter(!h3_id%in%c("831ec8fffffffff", "831ec9fffffffff", "832d05fffffffff"))

#For every h3_id, year,lat, and long keep only the first record
records_min=records %>% 
  group_by(h3_id,species) %>% 
  filter(year%in%min(year)) %>% 
 #For any remaining groups (h3_id/species) with multiple records in the minimum year, keep only the first one
  slice(1) %>%
#Ungroup the data (good practice after grouping operations)
  ungroup()

#Sum annual and spatial records
records_min_count=records_min %>% 
  group_by(h3_id,year) %>% 
  summarise(records=n()) %>% 
  rename(year_of_records=year)

#records_sp_level
records_min_sp_level=records %>% 
  group_by(h3_id,species) %>% 
  filter(year%in%min(year)) %>% 
 #For any remaining groups (h3_id/species) with multiple records in the minimum year, keep only the first one
  slice(1) %>%
#Ungroup the data (good practice after grouping operations)
  ungroup() %>% 
  group_by(h3_id,species,year) %>% 
  summarise(records=n()) %>% 
  rename(year_of_records=year)

```

##1.2.1) Plot records
Plot coords
```{r}
# Assuming your dataframe is named 'records'
# Ensure 'lat' and 'long' columns exist in 'records'

# 1. Get world map data
world_map <- rnaturalearth::ne_countries(scale = "large", returnclass = "sf")

# 2. Convert your 'records' data frame to an sf object
records_sf <- records_min %>%
  drop_na(lat, long) %>% # Remove rows with missing lat/long
  sf::st_as_sf(coords = c("long", "lat"), crs = 4326) # CRS 4326 is WGS84 for lat/long

# 3. Calculate the extent of your coordinates
bbox <- sf::st_bbox(records_sf)

# 4. Create the ggplot
rec_thinned_map=ggplot() +
  # Add the world map
  geom_sf(data = world_map, fill = "antiquewhite", color = "black", linewidth = 0.1) +
  # Add your data points
  geom_sf(data = records_sf, size = 1, alpha = 0.7) +
  # Crop the map to the extent of your points
  coord_sf(
    xlim = c(bbox["xmin"] - 1, bbox["xmax"] + 1), # Adjust buffer as needed
    ylim = c(bbox["ymin"] - 1, bbox["ymax"] + 1), # Adjust buffer as needed
    expand = FALSE # Prevents adding extra space around the crop
  ) +
  # Add labels and theme
  labs(
    title = "Species records (thinned)",
    x = "Longitude",
    y = "Latitude",
    color = "Species") +
  guides(color=F)+
  theme_bw() +
  theme(
# --- CHANGE IS HERE ---
 # Moves the legend inside the plot using (x, y) coordinates
legend.position = c(0.22, 0.17),
# Optional: Makes the legend background transparent
legend.background = element_rect(fill = "transparent", color = NA),
legend.key.size = unit(0.25, "cm"),
panel.grid.major = element_line(color = "skyblue", linewidth = 0.2),
    panel.background = element_rect(fill = "skyblue", color = NA) 
  )

# ggsave(filename ="rec_thinned_map.png" ,plot =rec_thinned_map ,device = "jpeg",units = "cm",width = 25,height = 12)
```

#1.3) Bind MHWs and records
Bind MHWs with the records of the succeeding year
```{r}
used_cells=read_rds("copernicus_data_by_hexagon/MEDSEA_MULTIYEAR_PHY_006_004_10m/corrected_hex_level_data/final_data/all_hexagons_distinct_cells_count.rds") %>% 
    mutate(h3_id = str_remove(h3_id, "_cleaned")) %>% 
  select(h3_id,n_distinct_cells)


c_int_records=annual_c_int %>% 
    filter(h3_id%in%records_min_count$h3_id %>% unique) %>% 
  left_join(records_min_count,
            by=c("h3_id","year_of_records")) %>% 
  mutate(records = replace_na(records, 0)) %>% 
  left_join(used_cells,by="h3_id") %>% 
  mutate(weights=log(n_distinct_cells))
#write.csv(c_int_records,"c_int_records_pseudo_first_fbl.csv",row.names = F)
```

Bind species level
```{r}
# 1. Ungroup the MHW dataset first to avoid data structure errors
annual_c_int_ungrouped <- annual_c_int %>% 
  ungroup()

# Get all unique species per h3_id from the records data
h3_species_combo <- records_min_sp_level %>%
  select(h3_id, species) %>%
  unique()

# Get the h3_id-year combinations along with annual_c_int from the UNGROUPED MHW data
h3_year_combo_mhw <- annual_c_int_ungrouped %>%
  select(h3_id, year_of_mhws, annual_c_int, year_of_records) %>%
  unique()

# 2. Perform the joins to create the final dataset
final_data_clean <- h3_year_combo_mhw %>%
  
  # Cross-join MHW data with all species associated with that h3_id
  # This creates every combination of (h3_id, year, species)
  left_join(h3_species_combo, by = "h3_id") %>%
  
  # Join with the records data to get the 'records' count (1 or NA)
  left_join(records_min_sp_level %>% select(h3_id, species, year_of_records, records), 
            by = c("h3_id", "species", "year_of_records")) %>%
  
  # Replace NA in records with 0
  mutate(records = replace_na(records, 0)) %>%
  
  # Ensure only the necessary columns remain and ordering is correct
  select(h3_id, species, year_of_mhws, annual_c_int, year_of_records, records) %>%
  
  # Ungroup again just to ensure the final output is a regular data frame
  ungroup()

######################################################################
#Now keep only mhw data preexisting to the species record in h3_id
filtered_data_before_first_record <- final_data_clean %>%
  # 1. Group by the unique identifier pairs
  group_by(h3_id, species) %>%
  
  # 2. Arrange the data chronologically (important for finding the 'first' record)
  arrange(year_of_records, .by_group = TRUE) %>%
  
  # 3. Determine the year of the first record (records = 1)
  #    - If records exist (sum(records) > 0), find the earliest year where records == 1.
  #    - If no record exists for this h3_id/species, use the maximum year in the data
  #      to ensure all rows are kept (as there is no 'first record' to stop at).
  mutate(
    first_record_year = if_else(
      sum(records) > 0,
      min(year_of_records[records == 1]),
      max(year_of_records) # Keep all years if the species was never recorded
    )
  ) %>%
  
  # 4. Filter: keep rows where the year is less than or equal to the year of the
  #    first record.
  filter(year_of_records <= first_record_year) %>%
  
  # 5. Remove the helper column and ungroup
  select(-first_record_year) %>%
  ungroup() %>%
  filter(!is.na(species)) %>% 
  group_by(h3_id,species) %>%
  filter(sum(records)>0) %>% 
  ungroup()


#Check that there are no species with zero sum across hexagons and years
filtered_data_before_first_record %>% 
  group_by(h3_id,species) %>% 
  summarise(sum=sum(records)) %>% view
#write_rds(filtered_data_before_first_record,"filtered_data_before_first_record.rds")
```

Explore
```{r}
ggplot(c_int_records)+
  geom_point(aes(x=year_of_records,y=records))

ggplot(c_int_records)+
  geom_point(aes(x=annual_c_int,y=records))

ggplot(c_int_records)+
  geom_point(aes(x=year_of_records,y=annual_c_int))
```

#1.4) GLMMs
##species_level
#2.1) detrended cumulative intensity

Estimate annual cumulative intensity per hexagon
```{r}
annual_c_int_dt=clim_dt %>% 
  drop_na(residuals) %>% 
  mutate(
    # Create a new column, e.g., 'intensity'
    intensity = dplyr::if_else(
      !is.na(event_no), # The condition: is event_no not NA?
      residuals - seas, # What to do if the condition is TRUE
      0 # What to do if the condition is FALSE
    )) %>% 
    mutate(year_of_mhws=lubridate::year(date)) %>% 
  group_by(h3_id,year_of_mhws) %>% 
  summarise(annual_c_int=sum(intensity)) %>% 
  mutate(year_of_records=year_of_mhws+1)

#write_rds(annual_c_int_dt,"annual_c_int_dt.rds")
```

#2.2) Bind MHWs and records
Bind MHWs with the records of the succeeding year
```{r}
#add used cells per hexagon
used_cells=read_rds("copernicus_data_by_hexagon/MEDSEA_MULTIYEAR_PHY_006_004_10m/corrected_hex_level_data/final_data/all_hexagons_distinct_cells_count.rds") %>% 
    mutate(h3_id = str_remove(h3_id, "_cleaned")) %>% 
  select(h3_id,n_distinct_cells)

c_int_records_dt=annual_c_int_dt %>% 
  filter(h3_id%in%records_min_count$h3_id %>% unique) %>% 
  left_join(records_min_count,
            by=c("h3_id","year_of_records")) %>% 
  mutate(records = replace_na(records, 0)) %>% 
  left_join(used_cells,by="h3_id") %>% 
  mutate(weights=log(n_distinct_cells))
#write.csv(c_int_records_dt,"c_int_records_pseudo_first_dt.csv",row.names = F)
```

Explore
```{r}
ggplot(c_int_records_dt)+
  geom_point(aes(x=year_of_records,y=records))

ggplot(c_int_records_dt)+
  geom_point(aes(x=annual_c_int,y=records))

ggplot(c_int_records_dt)+
  geom_point(aes(x=year_of_records,y=annual_c_int))
```

Bind species level
```{r}
# 1. Ungroup the MHW dataset first to avoid data structure errors
annual_c_int_ungrouped_dt <- annual_c_int_dt %>% 
  ungroup()

# Get all unique species per h3_id from the records data
h3_species_combo_dt <- records_min_sp_level %>%
  select(h3_id, species) %>%
  unique()

# Get the h3_id-year combinations along with annual_c_int_dt from the UNGROUPED MHW data
h3_year_combo_mhw_dt <- annual_c_int_ungrouped_dt %>%
  select(h3_id, year_of_mhws, annual_c_int, year_of_records) %>%
  unique()

# 2. Perform the joins to create the final dataset
final_data_clean_dt <- h3_year_combo_mhw_dt %>%
  
  # Cross-join MHW data with all species associated with that h3_id
  # This creates every combination of (h3_id, year, species)
  left_join(h3_species_combo_dt, by = "h3_id") %>%
  
  # Join with the records data to get the 'records' count (1 or NA)
  left_join(records_min_sp_level %>% select(h3_id, species, year_of_records, records), 
            by = c("h3_id", "species", "year_of_records")) %>%
  
  # Replace NA in records with 0
  mutate(records = replace_na(records, 0)) %>%
  
  # Ensure only the necessary columns remain and ordering is correct
  select(h3_id, species, year_of_mhws, annual_c_int, year_of_records, records) %>%
  
  # Ungroup again just to ensure the final output is a regular data frame
  ungroup()

######################################################################
#Now keep only mhw data preexisting to the species record in h3_id
filtered_data_before_first_record_dt <- final_data_clean_dt %>%
  # 1. Group by the unique identifier pairs
  group_by(h3_id, species) %>%
  
  # 2. Arrange the data chronologically (important for finding the 'first' record)
  arrange(year_of_records, .by_group = TRUE) %>%
  
  # 3. Determine the year of the first record (records = 1)
  #    - If records exist (sum(records) > 0), find the earliest year where records == 1.
  #    - If no record exists for this h3_id/species, use the maximum year in the data
  #      to ensure all rows are kept (as there is no 'first record' to stop at).
  mutate(
    first_record_year = if_else(
      sum(records) > 0,
      min(year_of_records[records == 1]),
      max(year_of_records) # Keep all years if the species was never recorded
    )
  ) %>%
  
  # 4. Filter: keep rows where the year is less than or equal to the year of the
  #    first record.
  filter(year_of_records <= first_record_year) %>%
  
  # 5. Remove the helper column and ungroup
  select(-first_record_year) %>%
  ungroup() %>%
  filter(!is.na(species)) %>% 
  group_by(h3_id,species) %>%
  filter(sum(records)>0) %>% 
  ungroup()

#Check that there are no species with zero sum across hexagons and years
filtered_data_before_first_record_dt %>% 
  group_by(h3_id,species) %>% 
  summarise(sum=sum(records)) %>% view
#write_rds(filtered_data_before_first_record_dt,"filtered_data_before_first_record_dt.rds")
```

#2.3) Add sst
```{r}
annual_sst=read_rds(
  "copernicus_data_by_hexagon/MEDSEA_MULTIYEAR_PHY_006_004_10m//corrected_hex_level_data/final_data/all_hexagons_daily_summary_10m.rds") %>%
  mutate(h3_id = str_remove(h3_id, "_cleaned"),
         year_of_mhws=lubridate::year(date)) %>% 
  group_by(h3_id,year_of_mhws) %>% 
             summarise(median_sst=median(mean_sst_10m))

sptaial_sst=read_rds(
  "copernicus_data_by_hexagon/MEDSEA_MULTIYEAR_PHY_006_004_10m//corrected_hex_level_data/final_data/all_hexagons_daily_summary_10m.rds") %>%
  mutate(h3_id = str_remove(h3_id, "_cleaned"),
         year_of_mhws=lubridate::year(date)) %>% 
  group_by(h3_id) %>% 
             summarise(median_s_sst=median(mean_sst_10m),
                       max_s_sst=max(mean_sst_10m),
                       min_s_sst=min(mean_sst_10m),
                       mean_s_sst=mean(mean_sst_10m))

#add the annual sst
filtered_data_before_first_record_dt=read_rds("filtered_data_before_first_record_dt.rds")
filtered_data_before_first_record_dt_sst=filtered_data_before_first_record_dt %>% 
  left_join(annual_sst,by=c("h3_id","year_of_mhws")) %>% 
  left_join(ormef_h3_id ,by=c("h3_id")) %>% 
  mutate(weights=1/sources)
#write_rds(filtered_data_before_first_record_dt_sst,"filtered_data_before_first_record_dt_sst.rds")
```

#Plot spatial SST
```{r}
# 2. Convert H3 IDs to Spatial Polygons (sf object)
sptaial_sst_sf <- sptaial_sst  %>% 
  # Pass the entire data frame and specify the H3 column name
  h3jsr::cell_to_polygon(
    input = .,               # '.' refers to the incoming data frame
    simple = FALSE           # Recommended for a tidy output
  ) %>%
  # The output is now a proper sf object, so st_set_crs works
  sf::st_set_crs(4326)

# 3. Get Mediterranean Land Boundaries for Context
# Function from rnaturalearth
world <-rnaturalearth:: ne_countries(scale = "large", returnclass = "sf")

# 4. Create the ggplot map
sst_map <- ggplot() +
    # Plot the H3 Hexagons (your data)
  geom_sf(
    data = sptaial_sst_sf,
    # Fills the hexagons based on the 'sources' count
    aes(fill = median_s_sst), 
    color = "black",     
    linewidth = 0.1,     
    alpha = 0.8          
  ) +
  # Add land boundaries first 
  geom_sf(data = world, fill = "antiquewhite", color = "black", linewidth = 0.1) +
  # Customize the color scale (from ggplot2 / viridis package, which tidyverse uses)
  scale_fill_viridis_c(
    name = "SST (Â°C)",
    option = "turbo"
  ) +
  # Set the map extent to focus on the Mediterranean Sea
  coord_sf(
    xlim = c(-6, 38),   
    ylim = c(26, 48),  
    expand = FALSE
  ) +
  # Apply a clean theme and labels
  labs(
    title = "A) Spatial SST",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_bw() +
  theme(
# --- CHANGE IS HERE ---
 # Moves the legend inside the plot using (x, y) coordinates
legend.position = c(0.15, 0.17),
# Optional: Makes the legend background transparent
legend.background = element_rect(fill = "transparent", color = NA),
legend.key.size = unit(0.25, "cm"),
panel.grid.major = element_line(color = "#DBDBDC", linewidth = 0.2),
    panel.background = element_rect(fill = "#DBDBDC", color = NA) 
  )
sst_map
ggsave(filename ="sst_map.png" ,plot =sst_map ,device = "jpeg",units = "cm",width = 25,height = 12)
```

#Spatial c_intensity
```{r}
spatial_c_int_dt=annual_c_int_dt  %>% 
  group_by(h3_id) %>% 
  select(h3_id,year_of_mhws,annual_c_int) %>% 
  summarise(max_c_int=max(annual_c_int))

# 2. Convert H3 IDs to Spatial Polygons (sf object)
spatial_c_int_dt_sf <- spatial_c_int_dt %>% 
  # Pass the entire data frame and specify the H3 column name
  h3jsr::cell_to_polygon(
    input = .,               # '.' refers to the incoming data frame
    simple = FALSE           # Recommended for a tidy output
  ) %>%
  # The output is now a proper sf object, so st_set_crs works
  sf::st_set_crs(4326)

# 3. Get Mediterranean Land Boundaries for Context
# Function from rnaturalearth
world <-rnaturalearth:: ne_countries(scale = "large", returnclass = "sf")

# 4. Create the ggplot map
max_c_int_map <- ggplot() +
    # Plot the H3 Hexagons (your data)
  geom_sf(
    data = spatial_c_int_dt_sf,
    # Fills the hexagons based on the 'sources' count
    aes(fill = max_c_int), 
    color = "black",     
    linewidth = 0.1,     
    alpha = 0.8          
  ) +
  # Add land boundaries first 
  geom_sf(data = world, fill = "antiquewhite", color = "black", linewidth = 0.1) +
  # Customize the color scale (from ggplot2 / viridis package, which tidyverse uses)
  scale_fill_viridis_c(
    name = "Max. cumulative intensity (Â°C-days)",
    option = "turbo"
  ) +
  # Set the map extent to focus on the Mediterranean Sea
  coord_sf(
    xlim = c(-6, 38),   
    ylim = c(26, 48),  
    expand = FALSE
  ) +
  # Apply a clean theme and labels
  labs(
    title = "B) Spatial cumulative intensity",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_bw() +
  theme(
# --- CHANGE IS HERE ---
 # Moves the legend inside the plot using (x, y) coordinates
legend.position = c(0.3, 0.17),
# Optional: Makes the legend background transparent
legend.background = element_rect(fill = "transparent", color = NA),
legend.key.size = unit(0.25, "cm"),
panel.grid.major = element_line(color = "#DBDBDC", linewidth = 0.2),
    panel.background = element_rect(fill = "#DBDBDC", color = NA) 
  )
max_c_int_map
ggsave(filename ="max_c_int_map.png" ,plot =max_c_int_map ,device = "jpeg",units = "cm",width = 25,height = 12)
```

Bind maps
```{r}
spatial_temp_panel=gridExtra::grid.arrange(
  sst_map,
  max_c_int_map,
  layout_matrix = rbind(c(1),
                        c(2)))
ggsave(filename ="spatial_temp_panel.png" ,plot =spatial_temp_panel ,device = "jpeg",units = "cm",width = 20,height = 20)
```

#2.4) GLMMs
###monitoring effort control
Account for monitoring effort by years and h3_id
This specifically accounts for the increased monitoring effort over time
```{r}
#Proxy for monitoring effort
ormef <- read.delim("C:\\Users\\User\\Desktop\\research\\data\\ORMEF\\89476.tsv") %>% 
  mutate(
    # 1. str_replace() changes all commas (",") in the string to a period (".")
    Decimal_Lat = str_replace(Decimal_Lat, ",", "."),
    Decimal_Long = str_replace(Decimal_Long, ",", ".")) %>%
  # 2. Use a second mutate() call to convert the new string format to a numeric type
  mutate(
    Decimal_Lat = as.numeric(Decimal_Lat),
    Decimal_Long = as.numeric(Decimal_Long),
    h3_id = h3jsr::point_to_cell(
    input = sf::st_as_sf(.,
                         coords = c("Decimal_Long", "Decimal_Lat"),
                         crs = 4326),
    res = 3))# %>%  #keep all sources even if non Lessepsian related
 # filter(Category %in%"EXOTIC CAN")

ormef_h3_id=ormef %>% 
  group_by(h3_id) %>% 
  summarise(sources=n_distinct(Reference))

ormef_h3_id_year=ormef %>% 
  group_by(h3_id,Year) %>% 
  summarise(sources=n_distinct(Reference))

ggplot(data=ormef_h3_id_year)+
  geom_point(aes(x=Year,y=sources))
```

Use only hexagons that are within both datasets.
```{r}
filtered_data_before_first_record_dt_sst=read_rds("filtered_data_before_first_record_dt_sst.rds")
h3_only_records=intersect(filtered_data_before_first_record_dt_sst$h3_id %>% unique(),ormef_h3_id_year$h3_id %>% unique)

filtered_data_before_first_record_dt_sst_meff=
  filtered_data_before_first_record_dt_sst %>%
  filter(h3_id%in%h3_only_records) %>% 
  select(-sources,-weights) %>% 
  left_join(ormef_h3_id_year %>% 
              rename(year_of_records=Year),
            by=c("h3_id","year_of_records")) %>% 
  mutate(sources=case_when(is.na(sources)~0,
                           TRUE~sources))

#####################################
#Consider  using the ORMEF data to predict sources across out study range
test_sources=glmmTMB(data=filtered_data_before_first_record_dt_sst_meff %>% 
                  filter(year_of_records%in%c(1988:2020)),
                formula = sources~year_of_records,
                family = "nbinom1")
summary(test_sources)
plot(DHARMa::simulateResiduals(test_sources))
performance::check_singularity(test_sources)
performance::check_convergence(test_sources)
MuMIn::r.squaredGLMM(test_sources)
sjPlot::plot_model(test_sources,type="eff",terms="year_of_records",show.data = T,jitter = 0.1)
test_sources_gg=ggeffects::ggpredict(test_sources,terms="year_of_records[1988:2020 by=1]") %>% 
  as_tibble() %>% 
  select(year_of_records=x,
         sources_pred=predicted,
         conf.low,
         conf.high)

#add predicted
filtered_data_before_first_record_dt_sst_meff=filtered_data_before_first_record_dt_sst_meff %>% 
  left_join(test_sources_gg %>% 
              select(year_of_records,
                     sources_pred),
            by="year_of_records")

#plot
source_temp=ggplot(filtered_data_before_first_record_dt_sst_meff %>% 
                     ungroup() %>% 
                     distinct(h3_id,year_of_records,sources))+
  geom_jitter(data=filtered_data_before_first_record_dt_sst_meff %>% 
                filter(year_of_records%in%c(1988:2020)),
              aes(x=year_of_records,y=sources),height = 0.1)+
  geom_ribbon(data=test_sources_gg,
              aes(x=year_of_records,
                  ymin=conf.low,
                  ymax=conf.high),
              alpha=0.5)+
  geom_line(data=test_sources_gg,
              aes(x=year_of_records,
                  y=sources_pred),linewidth=1,
            color="purple")+
  theme_bw()+
  labs(x="Year",y="Sources",title="B) Temporal trend")
source_temp

cor.test(filtered_data_before_first_record_dt_sst_meff$sources,filtered_data_before_first_record_dt_sst_meff$year_of_mhws)

#species
filtered_data_before_first_record_dt_sst_meff$species %>% unique
#hexagons
filtered_data_before_first_record_dt_sst_meff$h3_id %>% unique
#Records
filtered_data_before_first_record_dt_sst_meff %>% 
  ungroup() %>% 
  filter(records%in%1) %>% 
  distinct(h3_id,species,year_of_mhws,records)

#write the analysis data with sources
# write_rds(filtered_data_before_first_record_dt_sst_meff,"filtered_data_before_first_record_dt_sst_meff.rds")
```

Plot proxy to monitoring effort
```{r}
# 2. Convert H3 IDs to Spatial Polygons (sf object)
ormef_sf <- ormef_h3_id %>%
  filter(h3_id%in%intersect(filtered_data_before_first_record_dt_sst$h3_id %>% unique(),ormef_h3_id_year$h3_id %>% unique)) %>% 
  # Pass the entire data frame and specify the H3 column name
  h3jsr::cell_to_polygon(
    input = .,               # '.' refers to the incoming data frame
    simple = FALSE           # Recommended for a tidy output
  ) %>%
  # The output is now a proper sf object, so st_set_crs works
  sf::st_set_crs(4326)

# 3. Get Mediterranean Land Boundaries for Context
# Function from rnaturalearth
world <-rnaturalearth:: ne_countries(scale = "large", returnclass = "sf")

# 4. Create the ggplot map
h3_map <- ggplot() +
    # Plot the H3 Hexagons (your data)
  geom_sf(
    data = ormef_sf,
    # Fills the hexagons based on the 'sources' count
    aes(fill = sources), 
    color = "black",     
    linewidth = 0.1,     
    alpha = 0.8          
  ) +
  # Add land boundaries first 
  geom_sf(data = world, fill = "antiquewhite", color = "black", linewidth = 0.1) +
  # Customize the color scale (from ggplot2 / viridis package, which tidyverse uses)
  scale_fill_viridis_c(
    name = "Sources",
    option = "turbo"
  ) +
  # Set the map extent to focus on the Mediterranean Sea
  coord_sf(
    xlim = c(-6, 38),   
    ylim = c(26, 48),  
    expand = FALSE
  ) +
  # Apply a clean theme and labels
  labs(
    title = "A) Spatial distribution of sources",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_bw() +
  theme(
# --- CHANGE IS HERE ---
 # Moves the legend inside the plot using (x, y) coordinates
legend.position = c(0.22, 0.17),
# Optional: Makes the legend background transparent
legend.background = element_rect(fill = "transparent", color = NA),
legend.key.size = unit(0.25, "cm"),
panel.grid.major = element_line(color = "#DBDBDC", linewidth = 0.2),
    panel.background = element_rect(fill = "#DBDBDC", color = NA) 
  )
h3_map
ggsave(filename ="h3_map.png" ,plot =h3_map ,device = "jpeg",units = "cm",width = 25,height = 12)
```

bias grid
```{r}
bias_grid=gridExtra::grid.arrange(
  h3_map,
                        source_temp,
  layout_matrix = rbind(c(1, 1,1,1,1,1,1),
                        c(1, 1,1,1,1,1,1),
                        c(1, 1,1,1,1,1,1),# Row 1: P1 and P2
                        c(NA,2,2,2,2,2,NA),
                        c(NA,2,2,2,2,2,NA)))
ggsave(filename ="bias_grid.png" ,plot =bias_grid ,device = "jpeg",units = "cm",width = 20,height = 20)
```

###Test
Load the data
```{r}
filtered_data_before_first_record_dt_sst_meff=read_rds("filtered_data_before_first_record_dt_sst_meff.rds") %>%#add the DCI_p index 
  # left_join(read.csv("dci_index.csv") %>% 
  #             select(h3_id,
  #                    DCI_P),
  #           by="h3_id") %>% 
  # left_join(read.csv("cpi_data_with_C_all.csv") %>% 
  #             select(h3_id,
  #                    CPI),
  #           by = join_by(h3_id)) %>% 
  left_join(sptaial_sst,by="h3_id")

year_of_f_rec=read.csv("records data/cleaned/records_cleaned_harm_8_10_25.csv") %>% 
  group_by(species) %>% 
  summarise(first_rec_year=min(year))
  # ormef %>% 
  # group_by(Species) %>% 
  # summarise(first_rec_year=min(Year)) %>% 
  # rename(species=Species) %>% 
  # mutate(species=case_when(species%in%"Torquigener flavimaculosus"~"Torquigener hypselogeneion",
  #           species%in%"Chelon carinatus"~"Planiliza carinata",
  #           species%in%"Oxyurichthys petersi"~"Oxyurichthys petersii",
  #           species%in%"Chelon carinatus"~"Cryptocentrus caeruleopunctatus",
  #           species%in%"Hippocampus fuscus"~"Hippocampus kuda",
  #           TRUE~species))

filtered_data_before_first_record_dt_sst_meff=filtered_data_before_first_record_dt_sst_meff %>% 
  left_join(year_of_f_rec,
            by="species") 
# %>% 
#   mutate(first_rec_year=case_when(species%in%"Cryptocentrus caeruleopunctatus"~2014,
#                                   species%in%"Himantura leoparda"~2016,#consider 2008 - paper
#                                   species%in%"Sardinella gibbosa"~2008,
#                                   species%in%"Ablennes hians"~2018,
#                                   species%in%"Cryptocentrus steinhardti"~2012,
#                                   species%in%"Sargocentron caudimaculatum"~2021,
#                                   species%in%"Synodus randalli"~2023,
#                                   species%in%"Acanthurus xanthopterus"~,
#                                   TRUE~first_rec_year))

# species_yfr=filtered_data_before_first_record_dt_sst_meff %>% 
#   select(species,first_rec_year) %>% 
#   distinct(species,first_rec_year)

#write.csv(species_yfr,"species_yfr.csv",row.names = F)
```

Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~Pay attention that sources data is limited to 2022~ so I should trim my data too~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```{r}
filtered_data_before_first_record_dt_sst_meff_2020=filtered_data_before_first_record_dt_sst_meff %>% 
    filter(year_of_records<=2020)

check_zeros=filtered_data_before_first_record_dt_sst_meff_2020 %>% 
  group_by(h3_id,species) %>% 
  summarise(n=(sum(records))) %>% 
  filter(n>0)

model_data_2020_clean=filtered_data_before_first_record_dt_sst_meff_2020 %>% 
  inner_join(check_zeros %>% select(-n),
             by=c("h3_id","species"))

check_zeros2=model_data_2020_clean %>% 
  group_by(h3_id,species) %>% 
  summarise(n=(sum(records)))
###################################################
#If I filter 2020 I need to remmove complete zero time series#
###################################################
library(glmmTMB)
test_sp_dt_time_sst_mon_eff=glmmTMB(
  data = model_data_2020_clean,
  formula = records~
    scale(first_rec_year)+
    scale(CPI)+
    scale(sources)+
    scale(annual_c_int)+
    scale(year_of_mhws)+
    scale(median_sst)+
    scale(max_s_sst)+
    (1|h3_id)+
    (1|species),
  family = "binomial",
  control = glmmTMBControl(optimizer = optim,
                           optArgs = list(method = "BFGS",
                                          maxit = 10000)))
#Colinearity
##median_s_sst
 #            Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI
 #   scale(median_sst) 5.90 [5.77, 6.04]         2.43      0.17     [0.17, 0.17]
 # scale(median_s_sst) 6.03 [5.90, 6.18]         2.46      0.17     [0.16, 0.17]

##max_s_sst - no colinearity!

##min_s_sst - no colinearity!

##mean_s_sst
# Moderate Correlation
#               Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI
#  scale(median_sst) 5.36 [5.24, 5.48]         2.32      0.19     [0.18, 0.19]
#  scale(mean_s_sst) 5.44 [5.32, 5.57]         2.33      0.18     [0.18, 0.19]

#write_rds(test_sp_dt_time_sst_mon_eff,"test_sp_dt_time_sst_mon_eff_binomial_s_sst.rds")
# test_sp_dt_time_sst_mon_eff=read_rds("glmms\\test_sp_dt_time_sst_mon_eff_cpi/test_sp_dt_time_sst_mon_eff_cpi.rds")
summary(test_sp_dt_time_sst_mon_eff)
plot(DHARMa::simulateResiduals(test_sp_dt_time_sst_mon_eff))
performance::check_singularity(test_sp_dt_time_sst_mon_eff)
performance::check_convergence(test_sp_dt_time_sst_mon_eff)
performance::check_overdispersion(test_sp_dt_time_sst_mon_eff)
performance::check_collinearity(test_sp_dt_time_sst_mon_eff)
MuMIn::r.squaredGLMM(test_sp_dt_time_sst_mon_eff)
#Summarise table for the model
fixed_effects_df <- broom.mixed::tidy(test_sp_dt_time_sst_mon_eff, effects = "fixed") %>%
  # Select and rename columns to match the request
  select(
    Variable = term,
    Coefficient = estimate,
    `Std. Error` = std.error,
    `Z Value` = statistic,
    `P Value` = p.value
  ) %>%
  # Format numeric columns for clean presentation (e.g., 3 decimal places)
  mutate(
    Coefficient = round(Coefficient, 3),
    `Std. Error` = round(`Std. Error`, 3),
    `Z Value` = round(`Z Value`, 2),
    `P Value` = format.pval(`P Value`, digits = 3, eps = 0.0001)
  )
#random effects
random_effects_df <- broom.mixed::tidy(test_sp_dt_time_sst_mon_eff, effects = "ran_pars") 

#write.csv(fixed_effects_df,"fixed_effects_df_binomial.csv",row.names = F)

#Binomial model: AIC = 7687.6
#Betabinomial model: AIC = 7689.6
#Delta is 2 - models are quite similar - I am chossing base on r^2
```
R2 partition
```{r}
####################
#Variance partition#
#####################
#Run Hierarchical Variance Partitioning using glmm.hp
## The glmm.hp function calculates the Marginal R2 (R2m) and then decomposes
## it into the individual contribution of each fixed effect predictor using
## hierarchical partitioning (average shared variance method).
library(glmm.hp)
#semantics
data_to_use <- model_data_2020_clean %>%
  mutate(
    first_rec_year_sc = scale(first_rec_year)[, 1],
    CPI_sc = scale(CPI)[, 1],
    sources_sc = scale(sources)[, 1],
    annual_c_int_sc = scale(annual_c_int)[, 1],
    year_of_mhws_sc = scale(year_of_mhws)[, 1],
    median_sst_sc = scale(median_sst)[, 1],
    max_s_sst_sc=scale(max_s_sst)[,1])

#model
test_sp_dt_time_sst_mon_eff_sem=glmmTMB(
  data = data_to_use,
  formula = records~
    first_rec_year_sc+
    CPI_sc+
    sources_sc+
    annual_c_int_sc+
    year_of_mhws_sc+
    median_sst_sc+
    max_s_sst_sc+
    (1|h3_id)+
    (1|species),
  family = "binomial")#,
  #control = glmmTMBControl(optimizer = optim,
  #                         optArgs = list(method = "BFGS",
  #                                        maxit = 10000)))
                                          
summary(test_sp_dt_time_sst_mon_eff_sem)
performance::check_singularity(test_sp_dt_time_sst_mon_eff_sem)
performance::check_convergence(test_sp_dt_time_sst_mon_eff_sem)
performance::check_collinearity(test_sp_dt_time_sst_mon_eff_sem)
MuMIn::r.squaredGLMM(test_sp_dt_time_sst_mon_eff_sem)
DHARMa::simulateResiduals(test_sp_dt_time_sst_mon_eff_sem) %>% plot
performance::check_overdispersion(test_sp_dt_time_sst_mon_eff_sem)
#pred list
# iv=list(pred1="first_rec_year_sc",
#         pred2="CPI_sc",
#         pred3="sources_sc",
#         pred4="annual_c_int_sc",
#         pred5="year_of_mhws_sc",
#         pred6="median_sst_sc")

R2_partition_results <-glmm.hp(test_sp_dt_time_sst_mon_eff_sem,
                                #iv =iv,
                                type="R2") # Specify that I want adjusted R2 partitioning

R2_partition_results
write_rds(R2_partition_results,"R2_partition_results_binomial_spatial_sst.rds")

#Interpretation
##Unique - The R2 explained by the predictor alone, i.e., in a model containing only that predictor.

##Average.share - The R2 explained by the predictor jointly with other predictors, calculated as the average improvement in R2 across all possible models where that predictor is added. This is the primary metric for individual predictor importance.

##Individual - The total contribution of the predictor to the model Rm. This is the sum of its Unique contribution and its Average.share contribution: Unique+Average.share.

##I.perc(%)	- The percentage contribution of the predictor to the total Rm. Calculated as:
#Individual/Rm2 Ã—100%.

R2_partition_results_df=R2_partition_results[[2]] %>% 
  as_data_frame() %>% 
  rownames_to_column() %>% 
  rename(predictors=1,
         Unique =2,
         Average.share=3,
         Individual=4,
         `I.perc(%)`=5) %>% 
  mutate(predictors=case_when(predictors%in%"1"~"Year of first record",
                              predictors%in%"2"~"CPI",
                              predictors%in%"3"~"Sources",
                              predictors%in%"4"~"Cumulative intensity",
                              predictors%in%"5"~"Year",
                              predictors%in%"6"~"SST")) %>% 
  # ðŸ‘‡ ADD THIS PART TO SUM DOWN THE COLUMNS AND ADD A NEW ROW
  bind_rows(
    summarise(., 
      predictors = "", # Set the predictor name to an empty string (" ")
      across(
        .cols = c(Unique, Average.share, Individual, `I.perc(%)`), # Specify the columns to sum
        .fns = ~sum(., na.rm = TRUE) # Apply the sum function
      )
    )
  )
  
write.csv(R2_partition_results_df,"R2_partition_results_df_binomial_spatial_sst.csv",row.names = F)
```

Extract predictions
```{r}
test_sp_dt_time_sst_mon_eff_gg_cint=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("annual_c_int [0:183 by =1]"))

test_sp_dt_time_sst_mon_eff_gg_yr=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("year_of_mhws [1987:2020 by =1]"))

test_sp_dt_time_sst_mon_eff_gg_sst=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("median_sst [14:24 by = 0.5]"))

test_sp_dt_time_sst_mon_eff_gg_s_sst=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("max_s_sst [23.7:31.1 by = 0.5]"))

test_sp_dt_time_sst_mon_eff_gg_source=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("sources [0:6 by = 0.1]"))

# test_sp_dt_time_sst_mon_eff_gg_source=ggeffects::ggpredict(
#   test_sp_dt_time_sst_mon_eff,
#   terms=c("sources_pred [0.1:2 by = 0.01]"))

test_sp_dt_time_sst_mon_eff_gg_cpi=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("CPI [-0.09:0.09 by = 0.001]"))

test_sp_dt_time_sst_mon_eff_gg_yfr=ggeffects::ggpredict(
  test_sp_dt_time_sst_mon_eff,
  terms=c("first_rec_year [1924:2020 by = 1]"))

write_rds(test_sp_dt_time_sst_mon_eff_gg_cint,
          "test_sp_dt_time_sst_mon_eff_gg_cint.rds")
write_rds(test_sp_dt_time_sst_mon_eff_gg_yr,
          "test_sp_dt_time_sst_mon_eff_gg_yr.rds")
write_rds(test_sp_dt_time_sst_mon_eff_gg_sst,
          "test_sp_dt_time_sst_mon_eff_gg_sst.rds")
write_rds(test_sp_dt_time_sst_mon_eff_gg_source,
          "test_sp_dt_time_sst_mon_eff_gg_source.rds")
write_rds(test_sp_dt_time_sst_mon_eff_gg_cpi,
          "test_sp_dt_time_sst_mon_eff_gg_cpi.rds")
write_rds(test_sp_dt_time_sst_mon_eff_gg_yfr,
          "test_sp_dt_time_sst_mon_eff_gg_yfr.rds")
write_rds(test_sp_dt_time_sst_mon_eff_gg_s_sst,
          "test_sp_dt_time_sst_mon_eff_gg_s_sst.rds")
```

####Plot model
```{r}
#c_intensity
test_sp_dt_time_sst_mon_eff_cint_p=ggplot(filtered_data_before_first_record_dt_sst_meff_2020)+
  geom_point(aes(x=annual_c_int,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_cint,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_cint,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Cumulative intensity (Â°C-days)",
       y="Record status",
       title="C) MHWs")+
  theme_bw()+
  theme(axis.title.y = element_blank())


#c_intensity zoom
test_sp_dt_time_sst_mon_eff_cint_p_zoom=ggplot(filtered_data_before_first_record_dt_sst_meff_2020)+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_cint,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_cint,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Cumulative intensity (Â°C-days)",
       y="Record status")+
  theme_bw()+
  theme(axis.title.y = element_blank())+
coord_cartesian(ylim=c(0,.1))+
  scale_y_continuous(breaks = seq(0,0.1,by=0.05))


#embed cint
embeded_cint <- test_sp_dt_time_sst_mon_eff_cint_p +
  annotation_custom(ggplotGrob(test_sp_dt_time_sst_mon_eff_cint_p_zoom),
                    xmin = 50, xmax = 150,
                    ymin = 0.1, ymax = 0.9)


#years
test_sp_dt_time_sst_mon_eff_yr_p=ggplot(filtered_data_before_first_record_dt_sst_meff_2020)+
  geom_point(aes(x=year_of_mhws,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_yr,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_yr,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Year",
       y="Record status",
       title="A) Time")+
  theme_bw()+
  theme(axis.title.y = element_blank())


#SST
test_sp_dt_time_sst_mon_eff_sst_p=ggplot(filtered_data_before_first_record_dt_sst_meff_2020)+
  geom_point(aes(x=median_sst,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_sst,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_sst,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="SST (Â°C)",
       y="Record status",
       title="B) SST")+
  theme_bw()+
  theme(axis.title.y = element_blank())

#SST zoom
test_sp_dt_time_sst_mon_eff_sst_p_zoom=ggplot(filtered_data_before_first_record_dt_sst_meff_2020)+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_sst,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_sst,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="SST (Â°C)",
       y="Record status")+
  theme_bw()+
  coord_cartesian(ylim=c(0,.05))+
  scale_y_continuous(breaks = seq(0,0.05,by=0.025))+
  theme(axis.title = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA))

#embed sst
embeded_sst <- test_sp_dt_time_sst_mon_eff_sst_p +
  annotation_custom(ggplotGrob(test_sp_dt_time_sst_mon_eff_sst_p_zoom),
                    xmin = 15, xmax = 23,
                    ymin = 0.1, ymax = 0.9)+
  theme(axis.title.y = element_blank())

```

```{r}
test_sp_dt_time_sst_mon_eff_grid=gridExtra::grid.arrange(left="Probability of record occurrence",
  test_sp_dt_time_sst_mon_eff_yr_p,
                        embeded_sst,
                        test_sp_dt_time_sst_mon_eff_cint_p,
                        ncol = 2,
                        nrow=2,
  layout_matrix = rbind(c(1, 1,1,1), # Row 1: P1 and P2
                        c(2,2,3,3)))


ggsave(filename ="test_sp_dt_time_sst_mon_eff_grid.png" ,plot =test_sp_dt_time_sst_mon_eff_grid ,device = "jpeg",units = "cm",width = 20,height = 15)
```

Plot non-focal
```{r}
#Source
test_sp_dt_time_sst_mon_eff_source_p=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_point(aes(x=sources,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_source,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_source,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Sources",
       y="Record status",
       title="A) Sources")+
  theme_bw()+
  theme(axis.title.y = element_blank())

#CPI
test_sp_dt_time_sst_mon_eff_cpi_p=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_point(aes(x=CPI,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_cpi,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_cpi,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="CPI (m/s)",
       y="Probability of record occurrence",
       title="B) Current Propagation Index")+
  theme_bw()+
  theme(axis.title.y = element_blank())
test_sp_dt_time_sst_mon_eff_cpi_p

#Year of first record
test_sp_dt_time_sst_mon_eff_yfr_p=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_point(aes(x=first_rec_year,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_yfr,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_yfr,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Year of first record",
       y="Probability of record occurrence",
       title="C) Year of first record")+
  theme_bw()+
  theme(axis.title.y = element_blank())+
    scale_x_continuous(breaks = seq(1920,2020,by=20))
test_sp_dt_time_sst_mon_eff_yfr_p

#YFR zoom
test_sp_dt_time_sst_mon_eff_yfr_p_zoom=ggplot(filtered_data_before_first_record_dt_sst_meff)+
  geom_ribbon(data=test_sp_dt_time_sst_mon_eff_gg_yfr,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_sp_dt_time_sst_mon_eff_gg_yfr,
              aes(x=x,y=predicted),
            linewidth=1)+
  theme_bw()+
  coord_cartesian(ylim=c(0,.08))+
  scale_y_continuous(breaks = seq(0,0.08,by=0.04))+
  scale_x_continuous(breaks = seq(1920,2020,by=20))+
  theme(axis.title = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA))

#embed yfr
embeded_yfr <- test_sp_dt_time_sst_mon_eff_yfr_p +
  annotation_custom(ggplotGrob(test_sp_dt_time_sst_mon_eff_yfr_p_zoom),
                    xmin = 1940, xmax = 2000,
                    ymin = 0.1, ymax = 0.9)+
  theme(axis.title.y = element_blank())
```

Non climatic panel
```{r}
test_sp_dt_time_sst_mon_eff_grid_nc=gridExtra::grid.arrange(
  left="Probability of record occurrence",
  test_sp_dt_time_sst_mon_eff_source_p,
  test_sp_dt_time_sst_mon_eff_cpi_p,
  embeded_yfr,
  layout_matrix = rbind(c(1), # Row 1: P1 and P2
                        c(2),
                        c(3)))


ggsave(filename ="test_sp_dt_time_sst_mon_eff_grid_nc.png" ,plot =test_sp_dt_time_sst_mon_eff_grid_nc ,device = "jpeg",units = "cm",width = 15,height = 20)
```

###Plot used records
```{r}
# 2. Convert H3 IDs to Spatial Polygons (sf object)
used_records_sf <- filtered_data_before_first_record_dt_sst_meff %>%
  group_by(h3_id) %>%
  summarise(sum_records=sum(records)) %>% 
  # Pass the entire data frame and specify the H3 column name
  h3jsr::cell_to_polygon(
    input = .,               # '.' refers to the incoming data frame
    simple = FALSE           # Recommended for a tidy output
  ) %>%
  # The output is now a proper sf object, so st_set_crs works
  sf::st_set_crs(4326)

# 3. Get Mediterranean Land Boundaries for Context
# Function from rnaturalearth
world <-rnaturalearth:: ne_countries(scale = "large", returnclass = "sf")

# 4. Create the ggplot map
records_map <- ggplot() +
    # Plot the H3 Hexagons (your data)
  geom_sf(
    data = used_records_sf,
    # Fills the hexagons based on the 'sources' count
    aes(fill = sum_records), 
    color = "black",     
    linewidth = 0.1,     
    alpha = 0.8          
  ) +
  # Add land boundaries first 
  geom_sf(data = world, fill = "antiquewhite", color = "black", linewidth = 0.1) +
  # Customize the color scale (from ggplot2 / viridis package, which tidyverse uses)
  scale_fill_viridis_c(
    name = "Records",
    option = "turbo"
  ) +
  # Set the map extent to focus on the Mediterranean Sea
  coord_sf(
    xlim = c(-6, 38),   
    ylim = c(26, 48),  
    expand = FALSE
  ) +
  # Apply a clean theme and labels
  labs(
    title = "A) First records within the temoral range",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_bw() +
  theme(
# --- CHANGE IS HERE ---
 # Moves the legend inside the plot using (x, y) coordinates
legend.position = c(0.22, 0.17),
# Optional: Makes the legend background transparent
legend.background = element_rect(fill = "transparent", color = NA),
legend.key.size = unit(0.25, "cm"),
panel.grid.major = element_line(color = "skyblue", linewidth = 0.2),
    panel.background = element_rect(fill = "skyblue", color = NA) 
  )
records_map
ggsave(filename ="records_map.png" ,plot =records_map ,device = "jpeg",units = "cm",width = 25,height = 12)
```

sst over time
```{r}
sst_time=filtered_data_before_first_record_dt_sst_meff %>% 
  group_by(h3_id) %>% 
  distinct(h3_id,year_of_mhws,median_sst)
sst_time_p=ggplot(sst_time)+
  geom_point(aes(x=year_of_mhws,y=median_sst,color=median_sst),
             alpha=0.3)+
  geom_boxplot(aes(x=year_of_mhws,y=median_sst,group=year_of_mhws),
               alpha=0.2,notch=T,outliers = F)+
  geom_smooth(aes(x=year_of_mhws,y=median_sst),
              formula = y~x,method = "lm",se = F,color="black")+
  scale_color_viridis_c(
    name = "SST",
    option = "turbo"
  ) +
  labs(x="Years",y="Median SST (Â°C)",
       title="B) SST trend")+
  guides(color=F)+
  theme_bw()
sst_time_p
ggsave(filename ="sst_time_p.png" ,plot =sst_time_p ,device = "jpeg",units = "cm",width = 20,height = 12)
```

Cumulative intensity over time
```{r}
c_int_time=filtered_data_before_first_record_dt_sst_meff %>% 
  group_by(h3_id) %>% 
  distinct(h3_id,year_of_mhws,annual_c_int)

c_int_time_p=ggplot(c_int_time)+
  geom_point(aes(x=year_of_mhws,y=annual_c_int,color=annual_c_int),
             alpha=0.3)+
  geom_boxplot(aes(x=year_of_mhws,y=annual_c_int,group=year_of_mhws),
               alpha=0.2,notch=F,outliers = F)+
  scale_color_viridis_c(
    name = "Cumulative intensity",
    option = "turbo"
  ) +
  labs(x="Years",y="Cumulative intensity (Â°C-days)",
       title="C) MHWs detrended")+
  guides(color=F)+
  theme_bw()#+
  # theme(legend.position=c(0.85, 0.8),
  #       legend.background = element_blank(),
  #       legend.key.size = unit(0.4, 'cm'))
c_int_time_p
ggsave(filename ="c_int_time_p.png" ,plot =c_int_time_p ,device = "jpeg",units = "cm",width = 20,height = 12)
```

Into grid
```{r}
intro_grid=gridExtra::grid.arrange(
  records_map,
  sst_time_p,
  c_int_time_p,
  ncol = 2,
  nrow=2,
  layout_matrix = rbind(c(1,1,1,1,1,1), # Row 1: P1 and P2
                        c(1,1,1,1,1,1),
                        c(1,1,1,1,1,1),
                        c(2,2,2,3,3,3),
                        c(2,2,2,3,3,3)))

intro_grid
ggsave(filename ="intro_grid.png" ,plot =intro_grid ,device = "jpeg",units = "cm",width = 20,height = 20)
```


##dredge
```{r}
# Dredge searches all possible combinations of fixed effects.
# It automatically keeps the random effects ((1|h3_id) + (1|species)) in ALL models.

# 1.1 Set MuMIn's options to prevent NA's from failing the model
# This is crucial for dredge()
options(na.action = "na.fail")

# It also generates the null model (intercept only) and the global model (all fixed effects).
cat("\nPerforming Model Dredging (comparing all 64 models)...\n")
model_selection_table <-MuMIn::dredge(
  test_sp_dt_time_sst_mon_eff,
  rank = "AICc" # Use AIC corrected for small sample sizes
)

write_rds(model_selection_table,"model_selection_table_cut_2020.rds")
write.csv(model_selection_table %>% as_tibble(),"model_selection_table_cut_2020.csv",row.names = F)

# ==============================================================================
# STEP 3: MANUAL FILTERING AND AIC WEIGHT CALCULATION
# ==============================================================================
cat("\n-- Filtering and Recalculating Weights for Converged Models --\n")

# 3.1 Filter out all models that failed to converge (where AICc is NA)
valid_models <- subset(model_selection_table, !is.na(AICc))

# 3.2 Recalculate Delta AICc based ONLY on the converged models' best score
# This ensures delta=0 corresponds to the best CONVERGED model
min_AICc <- min(valid_models$AICc)
valid_models$delta <- valid_models$AICc - min_AICc

valid_models=valid_models %>% 
  as.data.frame()
valid_models$weight=as.numeric(valid_models$weight)
# 3.3 Manually Calculate Akaike Weights (Formula: exp(-0.5 * delta) / sum(exp(-0.5 * delta)))
# This is equivalent to MuMIn's weight() function, applied to the subset
valid_models=valid_models %>% 
  mutate(weight=exp(-0.5 * valid_models$delta) / sum(exp(-0.5 * valid_models$delta))) %>%
    mutate(across(where(is.numeric), ~ round(.x, 3)))

# Sort the valid models by the newly calculated AICc rank
valid_models <- valid_models[order(valid_models$AICc), ]
print(failed_models)

#write.csv(valid_models,"valid_selected_models.csv",row.names = F)

# ==============================================================================
# STEP 4: OUTPUT RESULTS
# ==============================================================================

cat("\n=======================================================\n")
cat(paste0(" MODEL SELECTION TABLE (N=", nrow(valid_models), " Converged Models) "))
cat("\n=======================================================\n")

# Print the top 10 *converged* models
print(head(valid_models, n = 10))

cat("\nInterpretation:\n")
cat(" - The AICc, delta, and weight columns are now only calculated for the 46 converged models.\n")
cat(" - delta: The difference in AICc between a model and the best CONVERGED model (delta=0).\n")
cat(" - weight: The Akaike weight, showing the probability of being the best model in this set.\n")

# Calculate variable importance (Sum of weights for each predictor)
# We must use the original sw() function on the VALID models
importance_ranking <- MuMIn::sw(valid_models)
cat("\n=======================================================\n")
cat("             RELATIVE VARIABLE IMPORTANCE              \n")
cat("=======================================================\n")
# Importance close to 1 indicates the variable is in all or most top-ranking models.
print(importance_ranking)

# Reset global options
options(na.action = "na.omit")
```

###dredge binomial <2020
```{r}
options(na.action = "na.fail")

# It also generates the null model (intercept only) and the global model (all fixed effects).
cat("\nPerforming Model Dredging (comparing all 64 models)...\n")
model_selection_table_binomial <-MuMIn::dredge(
  test_sp_dt_time_sst_mon_eff,
  rank = "AICc" # Use AIC corrected for small sample sizes
)

write_rds(model_selection_table_binomial,"model_selection_table_binomial_s_sst.rds")
write.csv(model_selection_table_binomial %>% as_tibble(),"model_selection_table_cut_2020_binomial_s_sst.csv",row.names = F)

options(na.action = "na.omit")

# Calculate variable importance (Sum of weights for each predictor)
# We must use the original sw() function on the VALID models
importance_ranking <- MuMIn::sw(model_selection_table_binomial)
importance_ranking %>% 
  as_tibble()
```


###test with DCIp
```{r}
test_sp_dt_time_sst_mon_eff_dcip=glmmTMB(
  data = filtered_data_before_first_record_dt_sst_meff,
  formula = records~
    scale(first_rec_year)+
    scale(DCI_P)+
    scale(sources)+
    scale(annual_c_int)+
    scale(year_of_mhws)+
    scale(median_sst)+
    (1|h3_id)+
    (1|species),
  family = "betabinomial")
test_sp_dt_time_sst_mon_eff_dcip %>% summary
```

####dredge - dcip
```{r}

# Dredge searches all possible combinations of fixed effects.
# It automatically keeps the random effects ((1|h3_id) + (1|species)) in ALL models.

# 1.1 Set MuMIn's options to prevent NA's from failing the model
# This is crucial for dredge()
options(na.action = "na.fail")

# It also generates the null model (intercept only) and the global model (all fixed effects).
cat("\nPerforming Model Dredging (comparing all 64 models)...\n")
model_selection_table_dcip <-MuMIn::dredge(
  test_sp_dt_time_sst_mon_eff_dcip,
  rank = "AICc" # Use AIC corrected for small sample sizes
)
model_selection_table_dcip
write_rds(model_selection_table_dcip,"model_selection_table_dcip.rds")
write.csv(model_selection_table_dcip %>% as_tibble(),"model_selection_table_dcip.csv",row.names = F)

# ==============================================================================
# STEP 3: MANUAL FILTERING AND AIC WEIGHT CALCULATION
# ==============================================================================
cat("\n-- Filtering and Recalculating Weights for Converged Models --\n")

# 3.1 Filter out all models that failed to converge (where AICc is NA)
valid_models_dcip <- subset(model_selection_table_dcip, !is.na(AICc))

# 3.2 Recalculate Delta AICc based ONLY on the converged models' best score
# This ensures delta=0 corresponds to the best CONVERGED model
min_AICc <- min(valid_models_dcip$AICc)
valid_models_dcip$delta <- valid_models_dcip$AICc - min_AICc

valid_models_dcip=valid_models_dcip %>% 
  as.data.frame()
valid_models_dcip$weight=as.numeric(valid_models_dcip$weight)
# 3.3 Manually Calculate Akaike Weights (Formula: exp(-0.5 * delta) / sum(exp(-0.5 * delta)))
# This is equivalent to MuMIn's weight() function, applied to the subset
valid_models_dcip=valid_models_dcip %>% 
  mutate(weight=exp(-0.5 * valid_models_dcip$delta) / sum(exp(-0.5 * valid_models_dcip$delta))) %>%
    mutate(across(where(is.numeric), ~ round(.x, 3)))

# Sort the valid models by the newly calculated AICc rank
valid_models_dcip <- valid_models_dcip[order(valid_models_dcip$AICc), ]
print(failed_models)

#write.csv(valid_models_dcip,"valid_selected_models_dcip.csv",row.names = F)

# ==============================================================================
# STEP 4: OUTPUT RESULTS
# ==============================================================================

cat("\n=======================================================\n")
cat(paste0(" MODEL SELECTION TABLE (N=", nrow(valid_models), " Converged Models) "))
cat("\n=======================================================\n")

# Print the top 10 *converged* models
print(head(valid_models_dcip, n = 10))

cat("\nInterpretation:\n")
cat(" - The AICc, delta, and weight columns are now only calculated for the 46 converged models.\n")
cat(" - delta: The difference in AICc between a model and the best CONVERGED model (delta=0).\n")
cat(" - weight: The Akaike weight, showing the probability of being the best model in this set.\n")

# Calculate variable importance (Sum of weights for each predictor)
# We must use the original sw() function on the VALID models
importance_ranking <- MuMIn::sw(valid_models_dcip)
cat("\n=======================================================\n")
cat("             RELATIVE VARIABLE IMPORTANCE              \n")
cat("=======================================================\n")
# Importance close to 1 indicates the variable is in all or most top-ranking models.
print(importance_ranking)

# Reset global options
options(na.action = "na.omit")
```

