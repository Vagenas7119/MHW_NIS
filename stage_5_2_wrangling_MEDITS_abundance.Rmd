---
title: "stage_5_wrangling_MEDITS"
author: "Shahar Chaikin"
date: "2025-10-01"
output: html_document
---

In this script I will clean the MEDITS data for our needs:

having year-over-year measure of NIS (and native) Biomass change.
This date could be later related with Annual MHWs cumulative intesity.

Libraries
```{r}
library(tidyverse)
```

#1) load data
```{r eval=FALSE, include=FALSE}
#Metadata and information on hauls
data_TA=read.csv("C:\\Users\\User\\Desktop\\research\\data\\MEDBSsurvey/Demersal/TA.csv")
#Catches by haul
data_TB=read.csv("C:\\Users\\User\\Desktop\\research\\data\\MEDBSsurvey/Demersal/TB.csv")
#Length and aggregated biological parameters
data_TC=read.csv("C:\\Users\\User\\Desktop\\research\\data\\MEDBSsurvey/Demersal/TC.csv")
#Taxa legened
taxa_data=data_TB %>% 
  mutate(medits_species=paste(genus,species,sep = "")) %>% 
  select(genus,species,medits_species) %>% 
  distinct(medits_species) %>% 
  left_join(read.csv("C:\\Users\\User\\Desktop\\research\\data\\MEDITS\\taxa_medits_fix_spaces_remove_NO.csv") %>% select(medits_species=1,
                 species=2),by="medits_species")
```

#Prepare medits files for use
##data_TA
Use only trawls that collect data for all species:
0 No standard species recorded
1 Only the species of the reference list are recorded
2 The species of the reference list plus some others are recorded
3 All the caught species are recorded
4 Species from a national list
```{r eval=FALSE, include=FALSE}
# Function to convert longitude to decimal degrees
convert_to_decimal_degrees <- function(longitude) {
  degrees <- floor(longitude / 100)
  minutes <- longitude %% 100
  decimal_degrees <- degrees + minutes / 60
  return(decimal_degrees)
}

#Prevelence of recorded species methodology
ggplot(data_TA)+
  geom_bar(aes(x=recorded_species))
ggplot(data_TA)+
  geom_bar(aes(x=validity)) #remain with valid trawls
#Filter and transform to decimal coords
data_TA_filtered=data_TA %>% 
  filter(recorded_species%in%3,#Use only trawls that collect all species
         validity%in%"V") %>%  #Use only trawls that are valid
  mutate(lat_haul_decimal=convert_to_decimal_degrees(hauling_latitude),
         long_haul_decimal=convert_to_decimal_degrees(hauling_longitude),#fix coordinate system to decimal degrees
         unique_haul_id=paste(paste(country,area,vessel,year,haul_number,sep = "_"))) %>%  #unique haul_id
  mutate(long_haul_decimal=case_when(area%in%c("1","2")~long_haul_decimal*-1,TRUE~long_haul_decimal))#Some trawls in area 1 (Alboran sea) received positive longitudes!
```

##data_TB
```{r eval=FALSE, include=FALSE}
data_TB_filtered = data_TB %>% 
  filter(catfau%in%c("A","Aa","Ae","Ao")) %>%
  #Only fish species (Aa, Ae, Ao)
  mutate(unique_haul_id=paste(paste(country,area,vessel,year,haul_number,sep = "_"))) %>% 
  #unique haul_id)
  left_join(data_TA_filtered %>% 
              select(unique_haul_id,lat_haul_decimal,long_haul_decimal,bottom_temperature_beginning,bottom_temperature_end,hauling_duration,depth=hauling_depth),
            by="unique_haul_id") %>% 
  mutate(lat_degree=sub("\\..*", "", lat_haul_decimal),
  #Extract numbers before the dot
         lon_degree=sub("\\..*", "", long_haul_decimal),
         medits_species=paste(genus,species,sep = "")) %>% 
  select(-species,genus) %>% 
  left_join(taxa_data,by="medits_species") %>% 
  mutate(hauling_duration_hours=hauling_duration/60,
         n_per_hour=nbtot/hauling_duration_hours) %>% 
  drop_na(n_per_hour,species) %>% 
  filter(n_per_hour>0)
```

#2) Remain with unflagged taxonomies and spatial biases
Think about this part
```{r eval=FALSE, include=FALSE}
data_TB_filtered_weight_outlier_remove_only_sp=
  data_TB_filtered %>% 
filter(!str_ends(species, "spp."),
       !str_detect(species, "idae$"))
```

#3) remove Outlier depths
remove poor depth representations over the years at the survey level

To be done later in the analysis stage
```{r eval=FALSE, include=FALSE}
#Use distinct depths per survey and year (and not species biased)
# distinct_depths_by_survey=data_TB_filtered %>% 
#   group_by(country,area,year) %>%
#   distinct(depth=as.factor(depth))
#plot
# ggplot() +
#   geom_boxplot(data = distinct_depths_by_survey,
#                aes(x = as.factor(year),
#                    y =as.numeric(depth))) +
#   facet_wrap(~ country, scales = "free_y") +
#   theme(axis.text.x = element_text(
#     angle = 90,
#     hjust = 1,
#     size = 7))

#remove regions with poor depth representations such as Croatia and Slovenia
# data_TB_filtered=data_TB_filtered %>% 
#   filter(!country%in%c("HRV","SVN"))
```

#4) create unique IDs

Assign a polygon for each sample
```{r}
#Add polygon IDs
data_TB_filtered_weight_outlier_remove_only_sp=data_TB_filtered_weight_outlier_remove_only_sp %>% 
  ungroup() %>% 
  mutate(h3_id = h3jsr::point_to_cell(
    input = sf::st_as_sf(.,
                         coords = c("long_haul_decimal", "lat_haul_decimal"),
                         crs = 4326),
    res = 3))

#Used polygons

h3_poly_used=data_TB_filtered_weight_outlier_remove_only_sp$h3_id %>% unique()
#write.csv(h3_poly_used,"h3_medit_abundance.csv")
# h3_poly_used=h3_poly %>% 
#   filter(address%in%unique(data_TB_filtered_weight_outlier_remove_only_sp$h3_id))
# write_rds(h3_poly_used,"h3_poly_used_MEDITS_biomass_new_data_version.rds")
# #plot
#raster::plot(h3_poly_used$polygons.all[1:46,])

#In case I wish to view on google earth
#sf::st_write(h3_poly_used, "h3_poly_used_MEDITS_biomass.kml", driver = "KML")
```

Impute zeros representing true non-detections
Group hauls into 1 degree lat and long per survey
```{r eval=FALSE, include=FALSE}
#Data cleaned with absences
data_cleaned_with_ab = data_TB_filtered_weight_outlier_remove_only_sp %>%
  select(country, unique_haul_id, h3_id, species, n_per_hour) %>%
  group_by(unique_haul_id, h3_id, country, species) %>%
  summarise(total_n_per_hour = 
              sum(n_per_hour)) %>%
  spread(key = species,
         value = total_n_per_hour,
         fill = 0) %>%
  #Impute zeros
  gather(key = species, value = total_n_per_hour, 4:ncol(.)) %>%
  mutate(unique_h3_population = paste(country,
                                      h3_id,
                                      species,
                                      sep = "_")) %>% #create unique population identifier by h3_id
  relocate(unique_h3_population) %>%
  left_join(
    data_TA_filtered %>%
      group_by(unique_haul_id) %>%
      distinct(
        year,
        month,
        day,
        depth = hauling_depth,
        area,
        sbt_start = bottom_temperature_beginning,
        sbt_end = bottom_temperature_end
      ),
    by = "unique_haul_id"
  )
```

#5) Spatio-temporal filtering
According to BioTIME, I will remove hauls representing grid cells that were sampled less than 4 times per yesr.
```{r}
#Remain with populations that were sampled at least four time per year
data_cleaned_with_ab_st_control = data_cleaned_with_ab %>%
  group_by(unique_h3_population, year) %>%
  filter(n_distinct(unique_haul_id) >= 4) %>%
  ungroup()

#How many h3_ids were lost?
h3_poly_used2=data_cleaned_with_ab_st_control$h3_id %>% unique()
setdiff(h3_poly_used,h3_poly_used2)
#write.csv(h3_poly_used2,"h3_medits2_abundance.csv")
```

#6) Controlling for zero abundance throughout the temporal range
```{r eval=FALSE, include=FALSE}
data_cleaned_with_ab_rm_zaty <- data_cleaned_with_ab_st_control %>%
  group_by(unique_h3_population) %>%
  filter(sum(total_n_per_hour) > 0) %>%
  ungroup()

#How many h3_ids were lost?  Non
h3_poly_used3=data_cleaned_with_ab_rm_zaty$h3_id %>% unique()
setdiff(h3_poly_used2,h3_poly_used3)
```

#7) Remove populations that were sampled for less than 10 years
I am going to skip this step as curently I am not insterested in long time series
```{r eval=FALSE, include=FALSE}
data_cleaned_with_ab_rm_zaty_duration <- data_cleaned_with_ab_rm_zaty %>%
  group_by(unique_h3_population) %>%
  filter(max(year) - min(year) >= 10) %>%
  ungroup()
```

Write the filtered data
```{r eval=FALSE, include=FALSE}
write_rds(x = data_cleaned_with_ab_rm_zaty,
          file = "cleaned_MEDITS_abundance.rds")
```

Read RDS
```{r}
data_cleaned_with_ab_rm_zaty=read_rds(file ="cleaned_MEDITS.rds")
```

#8) Populations summary
```{r}
#Populations summary
pop_summary=data_cleaned_with_ab_rm_zaty %>% 
    group_by(unique_h3_population,species,country) %>% 
    summarise(n_hauls=n_distinct(unique_haul_id),
              n_dis_years=n_distinct(year),
              min_year=min(year),
              max_year=max(year),
              temporal_range=(max_year-min_year)+1,
              temporal_completeness=(n_dis_years/temporal_range) %>% round(digits=3),
              n_dis_depths=n_distinct(depth),
              min_depth=min(depth),
              max_depth=max(depth),
              depth_range=max_depth-min_depth)

# write_csv(x = pop_summary,
#           file = "pop_summary_cleaned_MEDITS_abundance.csv")

pop_summary$temporal_range %>% hist
pop_summary$temporal_completeness %>% hist
```

remove RAM
```{r eval=FALSE, include=FALSE}
rm(data_cleaned_with_ab)
gc()
```

#9) Quality checks
Are ther populations with zero biomass?
```{r}
data_cleaned_with_ab_rm_zaty %>% 
  group_by(unique_h3_population) %>% 
  summarise(total_n=sum(total_n_per_hour)) %>% 
  pull(total_n) %>% 
  range
#No
```

Are there populations with less than 4 hauls per year?
```{r}
data_cleaned_with_ab_rm_zaty %>% 
  group_by(unique_h3_population,year) %>% 
  summarise(total_hauls=n_distinct(unique_haul_id)) %>% 
  pull(total_hauls) %>% 
  range
#No
```