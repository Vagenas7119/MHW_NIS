---
title: "stage_4_bind_mhw_records"
author: "Shahar Chaikin"
date: "2025-09-24"
output: html_document
---

Bind climatologies with records data

Libraries
```{r}
library(tidyverse)
library(glmmTMB)
library(glmm.hp)
```

#Data
```{r}
#data for models
data_models=read_rds("record_data_for_glmms.rds") %>% 
  mutate(time_series_id=paste(h3_id,species))
#for compatibility with variance partition
data_models_pre_scaled=data_models %>% 
  mutate(
    first_rec_year_sc = scale(first_rec_year)[, 1],
    CPI_sc = scale(CPI)[, 1],
    sources_sc = scale(sources)[, 1],
    annual_c_int_sc = scale(annual_c_int)[, 1],
    year_of_mhws_sc = scale(year_of_mhws)[, 1],
    median_sst_sc = scale(median_sst)[, 1],
    long_max_s_sst_sc=scale(long_max_s_sst)[,1])

data_models$time_series_id %>% n_distinct(
  
)
```

#1) Modeling
##GLMM
```{r}
test_main=glmmTMB(
  data = data_models,
  formula = records~
    scale(first_rec_year)+
    scale(CPI)+
    scale(sources)+
    scale(annual_c_int)+
    scale(year_of_mhws)+
    scale(median_sst)+
    scale(long_max_s_sst)+
    (1|h3_id)+
    (1|species),
  family = "binomial",
  control = glmmTMBControl(optimizer = optim,
                           optArgs = list(method = "BFGS",
                                          maxit = 10000)))
#Colinearity
##median_s_sst
 #            Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI
 #   scale(median_sst) 5.90 [5.77, 6.04]         2.43      0.17     [0.17, 0.17]
 # scale(median_s_sst) 6.03 [5.90, 6.18]         2.46      0.17     [0.16, 0.17]

##max_s_sst - no colinearity!

##min_s_sst - no colinearity!

##mean_s_sst
# Moderate Correlation
#               Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI
#  scale(median_sst) 5.36 [5.24, 5.48]         2.32      0.19     [0.18, 0.19]
#  scale(mean_s_sst) 5.44 [5.32, 5.57]         2.33      0.18     [0.18, 0.19]

#write_rds(test_main,"test_main.rds")
# test_main=read_rds("glmms\\test_main.rds")
summary(test_main)
performance::check_singularity(test_main)
performance::check_convergence(test_main)
performance::check_overdispersion(test_main)
performance::check_collinearity(test_main)
plot(DHARMa::simulateResiduals(test_main))
MuMIn::r.squaredGLMM(test_main)
#Summarise table for the model
fixed_effects_df <- broom.mixed::tidy(test_main, effects = "fixed") %>%
  # Select and rename columns to match the request
  select(
    Variable = term,
    Coefficient = estimate,
    `Std. Error` = std.error,
    `Z Value` = statistic,
    `P Value` = p.value
  ) %>%
  # Format numeric columns for clean presentation (e.g., 3 decimal places)
  mutate(
    Coefficient = round(Coefficient, 3),
    `Std. Error` = round(`Std. Error`, 3),
    `Z Value` = round(`Z Value`, 2),
    `P Value` = format.pval(`P Value`, digits = 3, eps = 0.0001)
  )
#random effects
random_effects_df <- broom.mixed::tidy(test_main, effects = "ran_pars") 

#write.csv(fixed_effects_df,"fixed_effects_df_binomial.csv",row.names = F)

#Binomial model: AIC = 7687.6
#Betabinomial model: AIC = 7689.6
#Delta is 2 - models are quite similar - I am chossing base on r^2
```

Extract predictions
```{r}
test_main_gg_cint=ggeffects::ggpredict(
  test_main,
  terms=c("annual_c_int [0:183 by =1]"))

test_main_gg_yr=ggeffects::ggpredict(
  test_main,
  terms=c("year_of_mhws [1987:2020 by =1]"))

test_main_gg_sst=ggeffects::ggpredict(
  test_main,
  terms=c("median_sst [14:24 by = 0.5]"))

test_main_gg_s_sst=ggeffects::ggpredict(
  test_main,
  terms=c("long_max_s_sst [22.4:29.7 by = 0.5]"))

test_main_gg_source=ggeffects::ggpredict(
  test_main,
  terms=c("sources [0:6 by = 0.1]"))

# test_main_gg_source=ggeffects::ggpredict(
#   test_main,
#   terms=c("sources_pred [0.1:2 by = 0.01]"))

test_main_gg_cpi=ggeffects::ggpredict(
  test_main,
  terms=c("CPI [-0.03:0.06 by = 0.001]"))

test_main_gg_yfr=ggeffects::ggpredict(
  test_main,
  terms=c("first_rec_year [1902:2018 by = 1]"))

write_rds(test_main_gg_cint,
          "test_main_gg_cint.rds")
write_rds(test_main_gg_yr,
          "test_main_gg_yr.rds")
write_rds(test_main_gg_sst,
          "test_main_gg_sst.rds")
write_rds(test_main_gg_source,
          "test_main_gg_source.rds")
write_rds(test_main_gg_cpi,
          "test_main_gg_cpi.rds")
write_rds(test_main_gg_yfr,
          "test_main_gg_yfr.rds")
write_rds(test_main_gg_s_sst,
          "test_main_gg_s_sst.rds")
```

####Plot model
```{r}
#c_intensity
test_main_cint_p=ggplot(data_models)+
  geom_point(aes(x=annual_c_int,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_main_gg_cint,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_main_gg_cint,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Cumulative intensity (°C-days)",
       y="Record status",
       title="C) MHWs")+
  theme_bw()+
  theme(axis.title.y = element_blank())


#c_intensity zoom
test_main_cint_p_zoom=ggplot(data_models)+
  geom_ribbon(data=test_main_gg_cint,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_main_gg_cint,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Cumulative intensity (°C-days)",
       y="Record status")+
  theme_bw()+
  theme(axis.title.y = element_blank())+
coord_cartesian(ylim=c(0,.1))+
  scale_y_continuous(breaks = seq(0,0.1,by=0.05))


#embed cint
embeded_cint <- test_main_cint_p +
  annotation_custom(ggplotGrob(test_main_cint_p_zoom),
                    xmin = 50, xmax = 150,
                    ymin = 0.1, ymax = 0.9)


#years
test_main_yr_p=ggplot(data_models)+
  geom_jitter(aes(x=year_of_mhws,y=records),
             alpha=0.3,
             color="grey",
             width = 0.1,
             height = 0.01)+
  geom_ribbon(data=test_main_gg_yr,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#56B52F")+
  geom_line(data=test_main_gg_yr,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Year",
       y="Record status",
       title="A) Time")+
  theme_bw()+
  theme(axis.title.y = element_blank())


#SST
test_main_sst_p=ggplot(data_models)+
  geom_point(aes(x=median_sst,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_main_gg_sst,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_main_gg_sst,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="SST-temporal (°C)",
       y="Record status",
       title="B) SST")+
  theme_bw()+
  theme(axis.title.y = element_blank())

#SST zoom
test_main_sst_p_zoom=ggplot(data_models)+
  geom_ribbon(data=test_main_gg_sst,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_main_gg_sst,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="SST-temporal (°C)",
       y="Record status")+
  theme_bw()+
  coord_cartesian(ylim=c(0,.05))+
  scale_y_continuous(breaks = seq(0,0.05,by=0.025))+
  theme(axis.title = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA))

#embed sst
embeded_sst <- test_main_sst_p +
  annotation_custom(ggplotGrob(test_main_sst_p_zoom),
                    xmin = 15, xmax = 23,
                    ymin = 0.1, ymax = 0.9)+
  theme(axis.title.y = element_blank())

```

```{r}
test_main_grid=gridExtra::grid.arrange(left="Occurrence probability",
  test_main_yr_p,
                        embeded_sst+
    labs(title="B) SST-temporal"),
                        test_main_cint_p,
                        ncol = 2,
                        nrow=2,
  layout_matrix = rbind(c(1, 1,1,1), # Row 1: P1 and P2
                        c(2,2,3,3)))


ggsave(filename ="test_main_grid.png" ,plot =test_main_grid ,device = "jpeg",units = "cm",width = 20,height = 15)
```

Plot non-focal
```{r}
#Source
test_main_source_p=ggplot(data_models)+
  geom_point(aes(x=sources,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_main_gg_source,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_main_gg_source,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Sources",
       y="Record status",
       title="A) Sources")+
  theme_bw()+
  theme(axis.title.y = element_blank())

#CPI
test_main_cpi_p=ggplot(data_models)+
  geom_point(aes(x=CPI,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_main_gg_cpi,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_main_gg_cpi,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="CPI (m/s)",
       y="Probability of record occurrence",
       title="B) Current Propagation Index")+
  theme_bw()+
  theme(axis.title.y = element_blank())
test_main_cpi_p

#Year of first record
test_main_yfr_p=ggplot(data_models)+
  geom_point(aes(x=first_rec_year,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=test_main_gg_yfr,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_main_gg_yfr,
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Year of first record",
       y="Probability of record occurrence",
       title="C) Year of first record")+
  theme_bw()+
  theme(axis.title.y = element_blank())+
    scale_x_continuous(breaks = seq(1902,2020,by=20))
test_main_yfr_p

#YFR zoom
test_main_yfr_p_zoom=ggplot(data_models)+
  geom_ribbon(data=test_main_gg_yfr,
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=test_main_gg_yfr,
              aes(x=x,y=predicted),
            linewidth=1)+
  theme_bw()+
  coord_cartesian(ylim=c(0,.08))+
  scale_y_continuous(breaks = seq(0,0.08,by=0.04))+
  scale_x_continuous(breaks = seq(1902,2020,by=20))+
  theme(axis.title = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA))

#embed yfr
embeded_yfr <- test_main_yfr_p +
  annotation_custom(ggplotGrob(test_main_yfr_p_zoom),
                    xmin = 1920, xmax = 1990,
                    ymin = 0.1, ymax = 0.9)+
  theme(axis.title.y = element_blank())
```

Non climatic panel
```{r}
test_main_grid_nc=gridExtra::grid.arrange(
  left="Probability of record occurrence",
  test_main_source_p,
  test_main_cpi_p,
  embeded_yfr,
  layout_matrix = rbind(c(1), # Row 1: P1 and P2
                        c(2),
                        c(3)))


ggsave(filename ="test_main_grid_nc.png" ,plot =test_main_grid_nc ,device = "jpeg",units = "cm",width = 15,height = 20)
```

#2) R2 partition
```{r}
####################
#Variance partition#
#####################
#Run Hierarchical Variance Partitioning using glmm.hp
## The glmm.hp function calculates the Marginal R2 (R2m) and then decomposes
## it into the individual contribution of each fixed effect predictor using
## hierarchical partitioning (average shared variance method).
#model
test_main_r_part=glmmTMB(
  data = data_models_pre_scaled,
  formula = records~
    first_rec_year_sc+
    CPI_sc+
    sources_sc+
    annual_c_int_sc+
    year_of_mhws_sc+
    median_sst_sc+
    long_max_s_sst_sc+
    (1|h3_id)+
    (1|species),
  family = "binomial")

R2_partition_results <-glmm.hp(test_main_r_part,
                                #iv =iv,
                                type="R2") # Specify that I want adjusted R2 partitioning

R2_partition_results
write_rds(R2_partition_results,"R2_partition_results_test_main.rds")

#Interpretation
##Unique - The R2 explained by the predictor alone, i.e., in a model containing only that predictor.

##Average.share - The R2 explained by the predictor jointly with other predictors, calculated as the average improvement in R2 across all possible models where that predictor is added. This is the primary metric for individual predictor importance.

##Individual - The total contribution of the predictor to the model Rm. This is the sum of its Unique contribution and its Average.share contribution: Unique+Average.share.

##I.perc(%)	- The percentage contribution of the predictor to the total Rm. Calculated as:
#Individual/Rm2 ×100%.


############UPDATE
#Save as csv
R2_partition_results_df=R2_partition_results[[2]] %>% 
  as_data_frame() %>% 
  rownames_to_column() %>% 
  rename(predictors=1,
         Unique =2,
         Average.share=3,
         Individual=4,
         `I.perc(%)`=5) %>% 
  mutate(predictors=case_when(predictors%in%"1"~"Year of first record",
                              predictors%in%"2"~"CPI",
                              predictors%in%"3"~"Sources",
                              predictors%in%"4"~"Cumulative intensity",
                              predictors%in%"5"~"Year",
                              predictors%in%"6"~"SST - spatiotemporal",
                              predictors%in%"7"~"SST - spatial")) %>% 
  # 👇 ADD THIS PART TO SUM DOWN THE COLUMNS AND ADD A NEW ROW
  bind_rows(
    summarise(., 
      predictors = "", # Set the predictor name to an empty string (" ")
      across(
        .cols = c(Unique, Average.share, Individual, `I.perc(%)`), # Specify the columns to sum
        .fns = ~sum(., na.rm = TRUE) # Apply the sum function
      )
    )
  )
  
write.csv(R2_partition_results_df,"R2_partition_results_df_test_main.csv",row.names = F)
```


#3) Model selection
Using dredge
```{r}
# Dredge searches all possible combinations of fixed effects.
# It automatically keeps the random effects ((1|h3_id) + (1|species)) in ALL models.

# 1.1 Set MuMIn's options to prevent NA's from failing the model
# This is crucial for dredge()
options(na.action = "na.fail")

# It also generates the null model (intercept only) and the global model (all fixed effects).
cat("\nPerforming Model Dredging (comparing all 64 models)...\n")
model_selection_table <-MuMIn::dredge(
  test_main,
  rank = "AICc" # Use AIC corrected for small sample sizes
)

cleaned_table <- as.data.frame(model_selection_table)

# Identify all columns that are of type 'numeric' or 'integer' to process
numeric_cols <- names(cleaned_table)[
  sapply(cleaned_table, function(x) is.numeric(x))
]

# 2. Apply rounding, convert to character, and replace NAs with NA_character_
# The 'format' function ensures trailing zeros are kept (e.g., 0.1 -> "0.100").
cleaned_table <- cleaned_table %>%
  mutate(across(all_of(numeric_cols), 
                ~ if_else(is.na(.), 
                          NA_character_, # Preserve NA as character for the next step
                          format(round(., 3), nsmall = 3))))

# 3. Use data.table::replace_na() (or just base R if preferred, as data.table's main
# function is replace_na, not just replace) or a similar targeted replacement.
# Since the request was for data.table::, here is a way using data.table syntax
# to perform the final step, although using base R's is.na() on the whole data.frame is cleaner here:

# Convert to data.table temporarily for the final NA replacement step
data.table::setDT(cleaned_table)

# Use data.table's powerful subsetting and replacement for all columns.
# We are replacing the character NAs (NA_character_) with "-"
for (j in 1:ncol(cleaned_table)) {
  data.table::set(cleaned_table, 
                  i = which(is.na(cleaned_table[[j]])), 
                  j = j, 
                  value = "-")
}

# Convert back to data.frame if you prefer
cleaned_table2 <- as.data.frame(cleaned_table) %>% 
  select(-1,-2) %>% 
  rename(`Cumulative intensity`=1,
         CPI=2,
         `Year of first record`=3,
         `SST - spatial`=4,
         `SST - spatiotemporal`=5,
         `Sources`=6,
         `Year`=7) %>% 
  mutate(Model=row_number()) %>%
  relocate(Model) %>% 
  as_tibble()

write.csv(cleaned_table2,"cleaned_table.csv",row.names = F)


importance_ranking <- MuMIn::sw(model_selection_table)
importance_ranking
sw_df_long <- data.frame(
  Predictor = names(importance_ranking),
  SW = as.numeric(importance_ranking),
  row.names = NULL) %>% 
  mutate(Predictor=case_when(Predictor%in%"cond(scale(sources))"~"Sources",
                             Predictor%in%"cond(scale(year_of_mhws))"~"Year",
                             Predictor%in%"cond(scale(first_rec_year))"~"Year of first record",
                             Predictor%in%"cond(scale(median_sst))"~"SST - spatiotemporal",
                             Predictor%in%"cond(scale(CPI))"~"CPI",
                             Predictor%in%"cond(scale(annual_c_int))"~"Cumulative intensity",
                             Predictor%in%"cond(scale(long_max_s_sst))"~"SST - spatial"),
         SW=SW %>% round(digits = 3))

write.csv(sw_df_long,"sw_df_long.csv")
# Reset global options
options(na.action = "na.omit")
```

#4) Phase analyses
Georgos ran the model - estimate variance partition

Data preparation
```{r}
#########################
#all species after 1987##
#########################
data_1987=data_models %>% 
  ungroup() %>% 
  #remove species that entered before 1987
  filter(first_rec_year>1987) %>% 
  mutate(time_series_id=paste(h3_id,species))

#Now get rid of rows representing species with zero sum of records
check_zeros_data_1987=data_1987 %>% 
  group_by(h3_id,species) %>% 
  summarise(n=(sum(records))) %>% 
  filter(n>0)

#applay cleaning on the data
data_1987=data_1987 %>% 
  inner_join(check_zeros_data_1987 %>% select(-n),
             by=c("h3_id","species")) %>% 
  ungroup()
#make sure it is cleaned
data_1987 %>% 
  group_by(h3_id,species) %>% 
  summarise(n=(sum(records))) %>% 
  select(n) %>% 
  pull %>% 
  range
data_1987$species %>% n_distinct()
data_1987$h3_id %>% n_distinct()
data_1987 %>% nrow()
##How many dis rec per sp?
data_1987 %>% 
  group_by(species) %>% 
  summarise(sum_rec=sum(records)) %>% 
  view

# --- Step 1: Calculate the Species-Wide Occurrence Rank and the Time Series Rank ---
data_events_grouped <- data_1987 %>%
  # Create the chronological rank of the '1' record across ALL hexagons for the species
  group_by(species) %>%
  arrange(year_of_records) %>%
  # This column only has a value > 0 on the '1' record for each time series
  mutate(species_occurrence_number = cumsum(records)) %>%
  ungroup() %>%
# Determine the single rank for the entire time series
  group_by(time_series_id) %>%
  # The maximum value of the previous column is the final rank of the '1' for this time series
  mutate(max_occurrence_rank = max(species_occurrence_number)) %>%
  ungroup()

###################
#Pre-establishment#
###################

#filter the right time series based on the species above
pre_est_data=data_events_grouped %>%
  # Filter to keep ALL rows (0s and 1s) from time series whose rank is 1, 2, or 3
  filter(max_occurrence_rank >= 1 & max_occurrence_rank <= 3)

pre_est_data$species %>% n_distinct()
pre_est_data$h3_id %>% n_distinct()
pre_est_data %>% nrow()

pre_est_data %>% 
  group_by(time_series_id) %>% 
  summarise(sum_r=sum(records)) %>% 
  pull(sum_r) %>% range

####################
#Post-establishment#
####################
#Find time series representing the first til third record in the med.
post_est_data=data_events_grouped %>%
  # Filter to keep ALL rows (0s and 1s) from time series whose rank is 4 or greater
  filter(max_occurrence_rank >= 4)

post_est_data$species %>% n_distinct()
post_est_data$h3_id %>% n_distinct()
post_est_data %>% nrow()

post_est_species %>% 
  group_by(time_series_id) %>% 
  summarise(sum_r=sum(records)) %>% 
  pull(sum_r) %>% range
```

##4.1) Only species with first records after 1987
```{r}
all_after_1987=glmmTMB(
  data = data_1987,
  formula = records~
    scale(first_rec_year)+
    scale(CPI)+
    scale(sources)+
    scale(annual_c_int)+
    scale(year_of_mhws)+
    scale(median_sst)+
    scale(long_max_s_sst)+
    (1|h3_id)+
    (1|species),
  family = "binomial",
  control = glmmTMBControl(optimizer = optim,
                           optArgs = list(method = "BFGS",
                                          maxit = 10000)))

summary(all_after_1987)
performance::check_singularity(all_after_1987)
performance::check_convergence(all_after_1987)
performance::check_overdispersion(all_after_1987)
performance::check_collinearity(all_after_1987)
plot(DHARMa::simulateResiduals(all_after_1987))
MuMIn::r.squaredGLMM(all_after_1987)

#Summarise table for the model
fixed_effects_df_after_1987 <- broom.mixed::tidy(all_after_1987, effects = "fixed") %>%
  # Select and rename columns to match the request
  select(
    Variable = term,
    Coefficient = estimate,
    `Std. Error` = std.error,
    `Z Value` = statistic,
    `P Value` = p.value
  ) %>%
  # Format numeric columns for clean presentation (e.g., 3 decimal places)
  mutate(
    Coefficient = round(Coefficient, 3),
    `Std. Error` = round(`Std. Error`, 3),
    `Z Value` = round(`Z Value`, 2),
    `P Value` = format.pval(`P Value`, digits = 3, eps = 0.0001)
  )
#random effects
random_effects_df <- broom.mixed::tidy(all_after_1987, effects = "ran_pars") 

#write.csv(fixed_effects_df_after_1987,"fixed_effects_df_after_1987_binomial.csv",row.names = F)
```

###4.1.1) R partition
```{r}
#Rename for glmm.ho func
data_1987_scaled=data_1987 %>% 
  mutate(
    first_rec_year_sc = scale(first_rec_year)[, 1],
    CPI_sc = scale(CPI)[, 1],
    sources_sc = scale(sources)[, 1],
    annual_c_int_sc = scale(annual_c_int)[, 1],
    year_of_mhws_sc = scale(year_of_mhws)[, 1],
    median_sst_sc = scale(median_sst)[, 1],
    long_max_s_sst_sc=scale(long_max_s_sst)[,1])

#model
all_after_1987_sc=glmmTMB(
  data = data_1987_scaled,
  formula = records~
    first_rec_year_sc+
    CPI_sc+
    sources_sc+
    annual_c_int_sc+
    year_of_mhws_sc+
    median_sst_sc+
    long_max_s_sst_sc+
    (1|h3_id)+
    (1|species),
  family = "binomial")

R2_partition_results_after_1987 <-glmm.hp(all_after_1987_sc,
                                #iv =iv,
                                type="R2") # Specify that I want adjusted R2 partitioning

R2_partition_results_after_1987
write_rds(R2_partition_results_after_1987,"R2_partition_results_after_1987_test_main.rds")

#Interpretation
##Unique - The R2 explained by the predictor alone, i.e., in a model containing only that predictor.

##Average.share - The R2 explained by the predictor jointly with other predictors, calculated as the average improvement in R2 across all possible models where that predictor is added. This is the primary metric for individual predictor importance.

##Individual - The total contribution of the predictor to the model Rm. This is the sum of its Unique contribution and its Average.share contribution: Unique+Average.share.

##I.perc(%)	- The percentage contribution of the predictor to the total Rm. Calculated as:
#Individual/Rm2 ×100%.


############UPDATE
#Save as csv
R2_partition_results_after_1987_df=R2_partition_results_after_1987[[2]] %>% 
  as_data_frame() %>% 
  rownames_to_column() %>% 
  rename(predictors=1,
         Unique =2,
         Average.share=3,
         Individual=4,
         `I.perc(%)`=5) %>% 
  mutate(predictors=case_when(predictors%in%"1"~"Year of first record",
                              predictors%in%"2"~"CPI",
                              predictors%in%"3"~"Sources",
                              predictors%in%"4"~"Cumulative intensity",
                              predictors%in%"5"~"Year",
                              predictors%in%"6"~"SST - spatiotemporal",
                              predictors%in%"7"~"SST - spatial")) %>% 
  # 👇 ADD THIS PART TO SUM DOWN THE COLUMNS AND ADD A NEW ROW
  bind_rows(
    summarise(., 
      predictors = "", # Set the predictor name to an empty string (" ")
      across(
        .cols = c(Unique, Average.share, Individual, `I.perc(%)`), # Specify the columns to sum
        .fns = ~sum(., na.rm = TRUE) # Apply the sum function
      )
    )
  )
  
write.csv(R2_partition_results_after_1987_df,"R2_partition_results_after_1987_df_test_main.csv",row.names = F)
```

##4.2) Pre-establishment
```{r}
pre_est_test=glmmTMB(
  data = pre_est_data,
  formula = records~
    scale(first_rec_year)+
    scale(CPI)+
    scale(sources)+
    scale(annual_c_int)+
    scale(year_of_mhws)+
    scale(median_sst)+
    scale(long_max_s_sst)+
    (1|h3_id)+
    (1|species),
  family = "binomial",
  control = glmmTMBControl(optimizer = optim,
                           optArgs = list(method = "BFGS",
                                          maxit = 10000)))

summary(pre_est_test)
performance::check_singularity(pre_est_test)
performance::check_convergence(pre_est_test)
performance::check_overdispersion(pre_est_test)
performance::check_collinearity(pre_est_test)
plot(DHARMa::simulateResiduals(pre_est_test))
MuMIn::r.squaredGLMM(pre_est_test)

#Summarise table for the model
fixed_effects_df_pre_est_test <- broom.mixed::tidy(pre_est_test, effects = "fixed") %>%
  # Select and rename columns to match the request
  select(
    Variable = term,
    Coefficient = estimate,
    `Std. Error` = std.error,
    `Z Value` = statistic,
    `P Value` = p.value
  ) %>%
  # Format numeric columns for clean presentation (e.g., 3 decimal places)
  mutate(
    Coefficient = round(Coefficient, 3),
    `Std. Error` = round(`Std. Error`, 3),
    `Z Value` = round(`Z Value`, 2),
    `P Value` = format.pval(`P Value`, digits = 3, eps = 0.0001)
  )
#random effects
pre_est_test_random_effects_df <- broom.mixed::tidy(pre_est_test, effects = "ran_pars") 

#write.csv(fixed_effects_df_pre_est_test,"fixed_effects_df_pre_est_test.csv",row.names = F)
```

###4.2.1) R partition
```{r}
#Rename for glmm.ho func
pre_est_data_scaled=pre_est_data %>% 
  mutate(
    first_rec_year_sc = scale(first_rec_year)[, 1],
    CPI_sc = scale(CPI)[, 1],
    sources_sc = scale(sources)[, 1],
    annual_c_int_sc = scale(annual_c_int)[, 1],
    year_of_mhws_sc = scale(year_of_mhws)[, 1],
    median_sst_sc = scale(median_sst)[, 1],
    long_max_s_sst_sc=scale(long_max_s_sst)[,1])

#model
pre_est_sc=glmmTMB(
  data = pre_est_data_scaled,
  formula = records~
    first_rec_year_sc+
    CPI_sc+
    sources_sc+
    annual_c_int_sc+
    year_of_mhws_sc+
    median_sst_sc+
    long_max_s_sst_sc+
    (1|h3_id)+
    (1|species),
  family = "binomial")

R2_partition_results_pre_est_sc <-glmm.hp(pre_est_sc,
                                #iv =iv,
                                type="R2") # Specify that I want adjusted R2 partitioning

R2_partition_results_pre_est_sc
write_rds(R2_partition_results_pre_est_sc,"R2_partition_results_pre_est_sc.rds")

#Interpretation
##Unique - The R2 explained by the predictor alone, i.e., in a model containing only that predictor.

##Average.share - The R2 explained by the predictor jointly with other predictors, calculated as the average improvement in R2 across all possible models where that predictor is added. This is the primary metric for individual predictor importance.

##Individual - The total contribution of the predictor to the model Rm. This is the sum of its Unique contribution and its Average.share contribution: Unique+Average.share.

##I.perc(%)	- The percentage contribution of the predictor to the total Rm. Calculated as:
#Individual/Rm2 ×100%.


############UPDATE
#Save as csv
R2_partition_results_pre_est_sc_df=R2_partition_results_pre_est_sc[[2]] %>% 
  as_data_frame() %>% 
  rownames_to_column() %>% 
  rename(predictors=1,
         Unique =2,
         Average.share=3,
         Individual=4,
         `I.perc(%)`=5) %>% 
  mutate(predictors=case_when(predictors%in%"1"~"Year of first record",
                              predictors%in%"2"~"CPI",
                              predictors%in%"3"~"Sources",
                              predictors%in%"4"~"Cumulative intensity",
                              predictors%in%"5"~"Year",
                              predictors%in%"6"~"SST - spatiotemporal",
                              predictors%in%"7"~"SST - spatial")) %>% 
  # 👇 ADD THIS PART TO SUM DOWN THE COLUMNS AND ADD A NEW ROW
  bind_rows(
    summarise(., 
      predictors = "", # Set the predictor name to an empty string (" ")
      across(
        .cols = c(Unique, Average.share, Individual, `I.perc(%)`), # Specify the columns to sum
        .fns = ~sum(., na.rm = TRUE) # Apply the sum function
      )
    )
  )
  
write.csv(R2_partition_results_pre_est_sc_df,"R2_partition_results_pre_est_sc_df.csv",row.names = F)
```

###4.2.2) extract predicted
```{r}
# List the predictors you want to marginalize over (from your model formula)
predictors <- c(
  "annual_c_int",
  "year_of_mhws",
  "median_sst",
  "long_max_s_sst",
  "sources",
  "CPI",
  "first_rec_year")

# Define the step size (by) for each predictor's range.
step_sizes <- list(
  annual_c_int = 1,
  year_of_mhws = 1,
  median_sst   = 0.5,
  long_max_s_sst = 0.5,
  sources      = 0.1,
  CPI          = 0.001,
  first_rec_year = 1)

# --- 2. Create the Data Frame of Prediction Terms ---

# Dynamically calculate the min/max range for each predictor from the original pre_est_test
prediction_terms_df <- pre_est_data %>%
  select(all_of(predictors)) %>%
  # Summarize the min and max for each column
  summarise(across(everything(), list(min = min, max = max))) %>%
  # Pivot to long format using names_pattern to capture the full predictor name
  pivot_longer(
    cols = everything(),
    names_to = c("predictor", ".value"),
    # The regex (.*) captures EVERYTHING up to the LAST underscore, and (min|max) captures the suffix.
    names_pattern = "(.*)_(min|max)"
  ) %>%
  # Add the 'by' step size. 'predictor' now contains the full name (e.g., 'annual_c_int'),
  # which correctly matches the names in the step_sizes list.
  mutate(by = unlist(step_sizes[predictor])) %>%
  # Construct the full 'terms' string for ggpredict()
  mutate(ggpredict_term = paste0(predictor, " [", min, ":", max, " by = ", by, "]"))

# --- 3. Automate Predictions and Combine Data Frames ---

all_predictions_combined_pre <- prediction_terms_df %>%
  pull(ggpredict_term) %>%
  map_dfr(~ {
    pred_data <-ggeffects::ggpredict(pre_est_test, terms = .x) %>% as.data.frame()
    predictor_name <- str_extract(.x, "^[^ ]+") # Extract the predictor name from the term string
    return(pred_data %>% mutate(predictor_used = predictor_name)) # Renamed to 'predictor_used' for clarity
  })
#write.csv(all_predictions_combined_pre,"all_predictions_combined_pre.csv",row.names = F)
```

##4.3) Post-establishment
```{r}
post_est_test=glmmTMB(
  data = post_est_data,
  formula = records~
    scale(first_rec_year)+
    scale(CPI)+
    scale(sources)+
    scale(annual_c_int)+
    scale(year_of_mhws)+
    scale(median_sst)+
    scale(long_max_s_sst)+
    (1|h3_id)+
    (1|species),
  family = "binomial",
  control = glmmTMBControl(optimizer = optim,
                           optArgs = list(method = "BFGS",
                                          maxit = 10000)))

summary(post_est_test)
performance::check_singularity(post_est_test)
performance::check_convergence(post_est_test)
performance::check_overdispersion(post_est_test)
performance::check_collinearity(post_est_test)
plot(DHARMa::simulateResiduals(post_est_test))
MuMIn::r.squaredGLMM(post_est_test)

#Summarise table for the model
fixed_effects_df_post_est_test <- broom.mixed::tidy(post_est_test, effects = "fixed") %>%
  # Select and rename columns to match the request
  select(
    Variable = term,
    Coefficient = estimate,
    `Std. Error` = std.error,
    `Z Value` = statistic,
    `P Value` = p.value
  ) %>%
  # Format numeric columns for clean presentation (e.g., 3 decimal places)
  mutate(
    Coefficient = round(Coefficient, 3),
    `Std. Error` = round(`Std. Error`, 3),
    `Z Value` = round(`Z Value`, 2),
    `P Value` = format.pval(`P Value`, digits = 3, eps = 0.0001)
  )
#random effects
post_est_test_random_effects_df <- broom.mixed::tidy(post_est_test, effects = "ran_pars") 

#write.csv(fixed_effects_df_post_est_test,"fixed_effects_df_post_est_test.csv",row.names = F)
```

###4.3.1) R partition
```{r}
#Rename for glmm.ho func
post_est_data_scaled=post_est_data %>% 
  mutate(
    first_rec_year_sc = scale(first_rec_year)[, 1],
    CPI_sc = scale(CPI)[, 1],
    sources_sc = scale(sources)[, 1],
    annual_c_int_sc = scale(annual_c_int)[, 1],
    year_of_mhws_sc = scale(year_of_mhws)[, 1],
    median_sst_sc = scale(median_sst)[, 1],
    long_max_s_sst_sc=scale(long_max_s_sst)[,1])

#model
post_est_sc=glmmTMB(
  data = post_est_data_scaled,
  formula = records~
    first_rec_year_sc+
    CPI_sc+
    sources_sc+
    annual_c_int_sc+
    year_of_mhws_sc+
    median_sst_sc+
    long_max_s_sst_sc+
    (1|h3_id)+
    (1|species),
  family = "binomial")

R2_partition_results_post_est_sc <-glmm.hp(post_est_sc,
                                #iv =iv,
                                type="R2") # Specify that I want adjusted R2 partitioning

R2_partition_results_post_est_sc
write_rds(R2_partition_results_post_est_sc,"R2_partition_results_post_est_sc.rds")

#Interpretation
##Unique - The R2 explained by the predictor alone, i.e., in a model containing only that predictor.

##Average.share - The R2 explained by the predictor jointly with other predictors, calculated as the average improvement in R2 across all possible models where that predictor is added. This is the primary metric for individual predictor importance.

##Individual - The total contribution of the predictor to the model Rm. This is the sum of its Unique contribution and its Average.share contribution: Unique+Average.share.

##I.perc(%)	- The percentage contribution of the predictor to the total Rm. Calculated as:
#Individual/Rm2 ×100%.


############UPDATE
#Save as csv
R2_partition_results_post_est_sc_df=R2_partition_results_post_est_sc[[2]] %>% 
  as_data_frame() %>% 
  rownames_to_column() %>% 
  rename(predictors=1,
         Unique =2,
         Average.share=3,
         Individual=4,
         `I.perc(%)`=5) %>% 
  mutate(predictors=case_when(predictors%in%"1"~"Year of first record",
                              predictors%in%"2"~"CPI",
                              predictors%in%"3"~"Sources",
                              predictors%in%"4"~"Cumulative intensity",
                              predictors%in%"5"~"Year",
                              predictors%in%"6"~"SST - spatiotemporal",
                              predictors%in%"7"~"SST - spatial")) %>% 
  # 👇 ADD THIS PART TO SUM DOWN THE COLUMNS AND ADD A NEW ROW
  bind_rows(
    summarise(., 
      predictors = "", # Set the predictor name to an empty string (" ")
      across(
        .cols = c(Unique, Average.share, Individual, `I.perc(%)`), # Specify the columns to sum
        .fns = ~sum(., na.rm = TRUE) # Apply the sum function
      )
    )
  )
  
write.csv(R2_partition_results_post_est_sc_df,"R2_partition_results_post_est_sc_df.csv",row.names = F)
```

###4.3.2) extract predicted
```{r}
# List the predictors you want to marginalize over (from your model formula)
predictors <- c(
  "annual_c_int",
  "year_of_mhws",
  "median_sst",
  "long_max_s_sst",
  "sources",
  "CPI",
  "first_rec_year")

# Define the step size (by) for each predictor's range.
step_sizes <- list(
  annual_c_int = 1,
  year_of_mhws = 1,
  median_sst   = 0.5,
  long_max_s_sst = 0.5,
  sources      = 0.1,
  CPI          = 0.001,
  first_rec_year = 1)

# --- 2. Create the Data Frame of Prediction Terms ---

# Dynamically calculate the min/max range for each predictor from the original pre_est_test
prediction_terms_df_post <- post_est_data %>%
  select(all_of(predictors)) %>%
  # Summarize the min and max for each column
  summarise(across(everything(), list(min = min, max = max))) %>%
  # Pivot to long format using names_pattern to capture the full predictor name
  pivot_longer(
    cols = everything(),
    names_to = c("predictor", ".value"),
    # The regex (.*) captures EVERYTHING up to the LAST underscore, and (min|max) captures the suffix.
    names_pattern = "(.*)_(min|max)"
  ) %>%
  # Add the 'by' step size. 'predictor' now contains the full name (e.g., 'annual_c_int'),
  # which correctly matches the names in the step_sizes list.
  mutate(by = unlist(step_sizes[predictor])) %>%
  # Construct the full 'terms' string for ggpredict()
  mutate(ggpredict_term = paste0(predictor, " [", min, ":", max, " by = ", by, "]"))

# --- 3. Automate Predictions and Combine Data Frames ---

all_predictions_combined_post <- prediction_terms_df_post %>%
  pull(ggpredict_term) %>%
  map_dfr(~ {
    pred_data <-ggeffects::ggpredict(post_est_test, terms = .x) %>% as.data.frame()
    predictor_name <- str_extract(.x, "^[^ ]+") # Extract the predictor name from the term string
    return(pred_data %>% mutate(predictor_used = predictor_name)) # Renamed to 'predictor_used' for clarity
  })

#write.csv(all_predictions_combined_post,"all_predictions_combined_post.csv",row.names = F)
```

##4.4) Plot phases
###4.4.1) Year
####pre
```{r}
pre_yr_p=ggplot(pre_est_data)+
  geom_jitter(aes(x=year_of_mhws,y=records),
             alpha=0.3,
             color="grey",
             width = 0.1,
             height = 0.01)+
  geom_ribbon(data=all_predictions_combined_pre %>% 
                filter(predictor_used%in%"year_of_mhws"),
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#56B52F")+
  geom_line(data=all_predictions_combined_pre %>% 
                filter(predictor_used%in%"year_of_mhws"),
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Year",
       y="Record status",
       title="A) ")+
  theme_bw()+
  theme(axis.title = element_blank())
pre_yr_p
```

####post
```{r}
post_yr_p=ggplot(post_est_data)+
  geom_jitter(aes(x=year_of_mhws,y=records),
             alpha=0.3,
             color="grey",
             width = 0.1,
             height = 0.01)+
  geom_ribbon(data=all_predictions_combined_post %>% 
                filter(predictor_used%in%"year_of_mhws"),
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#56B52F")+
  geom_line(data=all_predictions_combined_post %>% 
                filter(predictor_used%in%"year_of_mhws"),
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="Year",
       y="Record status",
       title="B) ")+
  theme_bw()+
  theme(axis.title = element_blank())
post_yr_p
```

###4.4.1) SST-spatiotemporal
####pre
```{r}
pre_sst_p=ggplot(pre_est_data)+
  geom_point(aes(x=median_sst,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=all_predictions_combined_pre %>% 
                filter(predictor_used%in%"median_sst"),
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=all_predictions_combined_pre %>% 
                filter(predictor_used%in%"median_sst"),
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="SST - spatiotemporal (°C)",
       y="Record status",
       title="C) ")+
  theme_bw()+
  theme(axis.title = element_blank())
pre_sst_p

#SST zoom
pre_sst_p_zoom=ggplot(pre_est_data)+
  geom_ribbon(data=all_predictions_combined_pre %>% 
                filter(predictor_used%in%"median_sst"),
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=all_predictions_combined_pre %>% 
                filter(predictor_used%in%"median_sst"),
              aes(x=x,y=predicted),
            linewidth=1)+
  theme_bw()+
  coord_cartesian(ylim=c(0,.08))+
  #scale_y_continuous(breaks = seq(0,0.05,by=0.025))+
  theme(axis.title = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA))

#embed sst
embeded_sst_pre <- pre_sst_p +
  annotation_custom(ggplotGrob(pre_sst_p_zoom),
                    xmin = 16, xmax = 23,
                    ymin = 0.1, ymax = 0.9)+
  theme(axis.title = element_blank())
embeded_sst_pre
```

####post
```{r}
post_sst_p=ggplot(post_est_data)+
  geom_point(aes(x=median_sst,y=records),
             alpha=0.3,
             color="grey")+
  geom_ribbon(data=all_predictions_combined_post %>% 
                filter(predictor_used%in%"median_sst"),
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=all_predictions_combined_post %>% 
                filter(predictor_used%in%"median_sst"),
              aes(x=x,y=predicted),
            linewidth=1)+
  labs(x="SST - spatiotemporal (°C)",
       y="Record status",
       title="D) ")+
  theme_bw()+
  theme(axis.title.y = element_blank())
post_sst_p

#SST zoom
post_sst_p_zoom=ggplot(post_est_data)+
  geom_ribbon(data=all_predictions_combined_post %>% 
                filter(predictor_used%in%"median_sst"),
              aes(x=x,ymin=conf.low,ymax=conf.high),
              alpha=0.3,
              fill="#E61316")+
  geom_line(data=all_predictions_combined_post %>% 
                filter(predictor_used%in%"median_sst"),
              aes(x=x,y=predicted),
            linewidth=1)+
  theme_bw()+
  coord_cartesian(ylim=c(0,.004))+
  #scale_y_continuous(breaks = seq(0,0.05,by=0.025))+
  theme(axis.title = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA))

#embed sst
embeded_sst_pro <- post_sst_p +
  annotation_custom(ggplotGrob(post_sst_p_zoom),
                    xmin = 15, xmax = 23,
                    ymin = 0.1, ymax = 0.9)+
  theme(axis.title = element_blank())
embeded_sst_pro
```

####Bind
```{r}
x_title_year <- textGrob("Year", gp = gpar(fontsize = 12))
x_title_sst <- textGrob("SST-temporal (°C)", gp = gpar(fontsize = 12))
l_col_title_pre <- textGrob("Pre-establishment", gp = gpar(fontsize = 12))
r_col_title_pre <- textGrob("Post-establishment", gp = gpar(fontsize = 12))


grid_pre_post_yr_sst=gridExtra::grid.arrange(
  left="Occurrence probability",
  l_col_title_pre,r_col_title_pre,
  pre_yr_p,post_yr_p,x_title_year,
  embeded_sst_pre,embeded_sst_pro,x_title_sst,
  layout_matrix = rbind(
    c(1, 2),
    c(3, 4),
    c(3, 4),
    c(3, 4),
    c(3, 4),
    c(3, 4),
    c(3, 4),
    c(3, 4),
    c(3, 4),
    c(3, 4),
    c(3, 4),# Top plots
    c(5, 5), # Title for top plots (spanning two columns)
    c(6, 7),
    c(6, 7),
    c(6, 7),
    c(6, 7),
    c(6, 7),
    c(6, 7),
    c(6, 7),
    c(6, 7),
    c(6, 7),
    c(6, 7),
    c(8,8)))


ggsave(filename ="grid_pre_post_yr_sst.png" ,plot =grid_pre_post_yr_sst ,device = "jpeg",units = "cm",width = 20,height = 15)
```

